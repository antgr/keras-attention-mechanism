{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dense_and_lstm_attention.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antgr/keras-attention-mechanism/blob/master/Dense_and_lstm_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYXL4Zdr1tMg",
        "colab_type": "text"
      },
      "source": [
        "## Attention utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcT7mYfe1rPX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras.backend as K\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4Vp1XZP1azH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b63b4f8a-1889-4826-99ec-8c75f16f18c6"
      },
      "source": [
        "def get_activations(model, inputs, print_shape_only=False, layer_name=None):\n",
        "    # Documentation is available online on Github at the address below.\n",
        "    # From: https://github.com/philipperemy/keras-visualize-activations\n",
        "    print('----- activations -----')\n",
        "    activations = []\n",
        "    inp = model.input\n",
        "    if layer_name is None:\n",
        "        outputs = [layer.output for layer in model.layers]\n",
        "    else:\n",
        "        outputs = [layer.output for layer in model.layers if layer.name == layer_name]  # all layer outputs\n",
        "    funcs = [K.function([inp] + [K.learning_phase()], [out]) for out in outputs]  # evaluation functions\n",
        "    layer_outputs = [func([inputs, 1.])[0] for func in funcs]\n",
        "    for layer_activations in layer_outputs:\n",
        "        activations.append(layer_activations)\n",
        "        if print_shape_only:\n",
        "            print(layer_activations.shape)\n",
        "        else:\n",
        "            print(layer_activations)\n",
        "    return activations\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CMg5c1s1d8k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(n, input_dim, attention_column=1):\n",
        "    \"\"\"\n",
        "    Data generation. x is purely random except that it's first value equals the target y.\n",
        "    In practice, the network should learn that the target = x[attention_column].\n",
        "    Therefore, most of its attention should be focused on the value addressed by attention_column.\n",
        "    :param n: the number of samples to retrieve.\n",
        "    :param input_dim: the number of dimensions of each element in the series.\n",
        "    :param attention_column: the column linked to the target. Everything else is purely random.\n",
        "    :return: x: model inputs, y: model targets\n",
        "    \"\"\"\n",
        "    x = np.random.standard_normal(size=(n, input_dim))\n",
        "    y = np.random.randint(low=0, high=2, size=(n, 1))\n",
        "    x[:, attention_column] = y[:, 0]\n",
        "    return x, y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8yOuV7n1mYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data_recurrent(n, time_steps, input_dim, attention_column=10):\n",
        "    \"\"\"\n",
        "    Data generation. x is purely random except that it's first value equals the target y.\n",
        "    In practice, the network should learn that the target = x[attention_column].\n",
        "    Therefore, most of its attention should be focused on the value addressed by attention_column.\n",
        "    :param n: the number of samples to retrieve.\n",
        "    :param time_steps: the number of time steps of your series.\n",
        "    :param input_dim: the number of dimensions of each element in the series.\n",
        "    :param attention_column: the column linked to the target. Everything else is purely random.\n",
        "    :return: x: model inputs, y: model targets\n",
        "    \"\"\"\n",
        "    x = np.random.standard_normal(size=(n, time_steps, input_dim))\n",
        "    y = np.random.randint(low=0, high=2, size=(n, 1))\n",
        "    x[:, attention_column, :] = np.tile(y[:], (1, input_dim))\n",
        "    return x, y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VonSMOuZ129M",
        "colab_type": "text"
      },
      "source": [
        "## Dense attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhbasTkh1oZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "#from attention_utils import get_activations, get_data\n",
        "\n",
        "np.random.seed(1337)  # for reproducibility\n",
        "from keras.models import *\n",
        "#from keras.layers import Input, Dense, merge\n",
        "from keras.layers import Input, Dense,multiply\n",
        "\n",
        "input_dim = 32\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Enk3GIP716T9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "    inputs = Input(shape=(input_dim,))\n",
        "\n",
        "    # ATTENTION PART STARTS HERE\n",
        "    attention_probs = Dense(input_dim, activation='softmax', name='attention_vec')(inputs)\n",
        "    #attention_mul = merge([inputs, attention_probs], output_shape=32, name='attention_mul', mode='mul')\n",
        "    attention_mul = multiply([inputs, attention_probs],name='attention_mul')\n",
        "    # ATTENTION PART FINISHES HERE\n",
        "\n",
        "    attention_mul = Dense(64)(attention_mul)\n",
        "    output = Dense(1, activation='sigmoid')(attention_mul)\n",
        "    model = Model(input=[inputs], output=output)\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KR7q9NtD1-ME",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1604
        },
        "outputId": "ddf70e01-d018-415c-8f9d-05893167694f"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    N = 10000\n",
        "    inputs_1, outputs = get_data(N, input_dim)\n",
        "\n",
        "    m = build_model()\n",
        "    m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    print(m.summary())\n",
        "\n",
        "    m.fit([inputs_1], outputs, epochs=20, batch_size=64, validation_split=0.5)\n",
        "\n",
        "    testing_inputs_1, testing_outputs = get_data(1, input_dim)\n",
        "\n",
        "    # Attention vector corresponds to the second matrix.\n",
        "    # The first one is the Inputs output.\n",
        "    attention_vector = get_activations(m, testing_inputs_1,\n",
        "                                       print_shape_only=True,\n",
        "                                       layer_name='attention_vec')[0].flatten()\n",
        "    print('attention =', attention_vector)\n",
        "\n",
        "    # plot part.\n",
        "    import matplotlib.pyplot as plt\n",
        "    import pandas as pd\n",
        "\n",
        "    pd.DataFrame(attention_vector, columns=['attention (%)']).plot(kind='bar',\n",
        "                                                                   title='Attention Mechanism as '\n",
        "                                                                         'a function of input'\n",
        "                                                                         ' dimensions.')\n",
        "    plt.show()\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "attention_vec (Dense)           (None, 32)           1056        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "attention_mul (Multiply)        (None, 32)           0           input_1[0][0]                    \n",
            "                                                                 attention_vec[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 64)           2112        attention_mul[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            65          dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 3,233\n",
            "Trainable params: 3,233\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 5000 samples, validate on 5000 samples\n",
            "Epoch 1/20\n",
            "5000/5000 [==============================] - 1s 182us/step - loss: 0.6821 - acc: 0.6240 - val_loss: 0.6697 - val_acc: 0.6976\n",
            "Epoch 2/20\n",
            "5000/5000 [==============================] - 0s 28us/step - loss: 0.6378 - acc: 0.7474 - val_loss: 0.6003 - val_acc: 0.7844\n",
            "Epoch 3/20\n",
            "5000/5000 [==============================] - 0s 30us/step - loss: 0.5413 - acc: 0.7994 - val_loss: 0.4925 - val_acc: 0.8162\n",
            "Epoch 4/20\n",
            "5000/5000 [==============================] - 0s 38us/step - loss: 0.4277 - acc: 0.8406 - val_loss: 0.3840 - val_acc: 0.8660\n",
            "Epoch 5/20\n",
            "5000/5000 [==============================] - 0s 37us/step - loss: 0.3136 - acc: 0.8908 - val_loss: 0.2699 - val_acc: 0.9032\n",
            "Epoch 6/20\n",
            "5000/5000 [==============================] - 0s 36us/step - loss: 0.1978 - acc: 0.9482 - val_loss: 0.1558 - val_acc: 0.9600\n",
            "Epoch 7/20\n",
            "5000/5000 [==============================] - 0s 35us/step - loss: 0.0980 - acc: 0.9850 - val_loss: 0.0690 - val_acc: 0.9900\n",
            "Epoch 8/20\n",
            "5000/5000 [==============================] - 0s 33us/step - loss: 0.0389 - acc: 0.9982 - val_loss: 0.0278 - val_acc: 0.9984\n",
            "Epoch 9/20\n",
            "5000/5000 [==============================] - 0s 29us/step - loss: 0.0154 - acc: 0.9998 - val_loss: 0.0127 - val_acc: 0.9994\n",
            "Epoch 10/20\n",
            "5000/5000 [==============================] - 0s 35us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.0071 - val_acc: 1.0000\n",
            "Epoch 11/20\n",
            "5000/5000 [==============================] - 0s 34us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 1.0000\n",
            "Epoch 12/20\n",
            "5000/5000 [==============================] - 0s 28us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.0032 - val_acc: 1.0000\n",
            "Epoch 13/20\n",
            "5000/5000 [==============================] - 0s 28us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.0024 - val_acc: 1.0000\n",
            "Epoch 14/20\n",
            "5000/5000 [==============================] - 0s 28us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 1.0000\n",
            "Epoch 15/20\n",
            "5000/5000 [==============================] - 0s 28us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
            "Epoch 16/20\n",
            "5000/5000 [==============================] - 0s 29us/step - loss: 9.9482e-04 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 1.0000\n",
            "Epoch 17/20\n",
            "5000/5000 [==============================] - 0s 29us/step - loss: 8.2858e-04 - acc: 1.0000 - val_loss: 0.0010 - val_acc: 1.0000\n",
            "Epoch 18/20\n",
            "5000/5000 [==============================] - 0s 31us/step - loss: 7.0243e-04 - acc: 1.0000 - val_loss: 8.6286e-04 - val_acc: 1.0000\n",
            "Epoch 19/20\n",
            "5000/5000 [==============================] - 0s 27us/step - loss: 6.0340e-04 - acc: 1.0000 - val_loss: 7.4424e-04 - val_acc: 1.0000\n",
            "Epoch 20/20\n",
            "5000/5000 [==============================] - 0s 28us/step - loss: 5.2359e-04 - acc: 1.0000 - val_loss: 6.4773e-04 - val_acc: 1.0000\n",
            "----- activations -----\n",
            "(1, 32)\n",
            "attention = [0.01149122 0.52731323 0.00761567 0.01147714 0.02416754 0.01202238\n",
            " 0.04293125 0.00711575 0.02110231 0.01609351 0.00861825 0.0104695\n",
            " 0.00968474 0.01577906 0.02046585 0.01528684 0.00683184 0.01050764\n",
            " 0.01013713 0.00952316 0.01206056 0.04242776 0.01707092 0.00783806\n",
            " 0.01008466 0.01662973 0.01569531 0.01542907 0.00846866 0.02052671\n",
            " 0.02435404 0.01078054]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAELCAYAAAAiIMZEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH9kZGOP2kKL",
        "colab_type": "text"
      },
      "source": [
        "## lstm attention|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf0N6G_i2DGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import multiply\n",
        "from keras.layers.core import *\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.models import *\n",
        "\n",
        "#from attention_utils import get_activations, get_data_recurrent\n",
        "\n",
        "INPUT_DIM = 2\n",
        "TIME_STEPS = 20\n",
        "# if True, the attention vector is shared across the input_dimensions where the attention is applied.\n",
        "SINGLE_ATTENTION_VECTOR = False\n",
        "APPLY_ATTENTION_BEFORE_LSTM = False\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPediNIq2pAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def attention_3d_block(inputs):\n",
        "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
        "    input_dim = int(inputs.shape[2])\n",
        "    a = Permute((2, 1))(inputs)\n",
        "    a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n",
        "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
        "    if SINGLE_ATTENTION_VECTOR:\n",
        "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
        "        a = RepeatVector(input_dim)(a)\n",
        "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
        "    #output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
        "    output_attention_mul = multiply([inputs, a_probs], name='attention_mul')\n",
        "    return output_attention_mul\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vV8J4ATQ2tEs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_attention_applied_after_lstm():\n",
        "    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))\n",
        "    lstm_units = 32\n",
        "    lstm_out = LSTM(lstm_units, return_sequences=True)(inputs)\n",
        "    attention_mul = attention_3d_block(lstm_out)\n",
        "    attention_mul = Flatten()(attention_mul)\n",
        "    output = Dense(1, activation='sigmoid')(attention_mul)\n",
        "    model = Model(input=[inputs], output=output)\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsS6Xade2vAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_attention_applied_before_lstm():\n",
        "    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))\n",
        "    attention_mul = attention_3d_block(inputs)\n",
        "    lstm_units = 32\n",
        "    attention_mul = LSTM(lstm_units, return_sequences=False)(attention_mul)\n",
        "    output = Dense(1, activation='sigmoid')(attention_mul)\n",
        "    model = Model(input=[inputs], output=output)\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZESW_AO2wjg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32152
        },
        "outputId": "673062be-8156-4b56-8fef-78820e3ea264"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    N = 300000\n",
        "    # N = 300 -> too few = no training\n",
        "    inputs_1, outputs = get_data_recurrent(N, TIME_STEPS, INPUT_DIM)\n",
        "\n",
        "    if APPLY_ATTENTION_BEFORE_LSTM:\n",
        "        m = model_attention_applied_before_lstm()\n",
        "    else:\n",
        "        m = model_attention_applied_after_lstm()\n",
        "\n",
        "    m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    print(m.summary())\n",
        "\n",
        "    m.fit([inputs_1], outputs, epochs=1, batch_size=64, validation_split=0.1)\n",
        "\n",
        "    attention_vectors = []\n",
        "    for i in range(300):\n",
        "        testing_inputs_1, testing_outputs = get_data_recurrent(1, TIME_STEPS, INPUT_DIM)\n",
        "        attention_vector = np.mean(get_activations(m,\n",
        "                                                   testing_inputs_1,\n",
        "                                                   print_shape_only=True,\n",
        "                                                   layer_name='attention_vec')[0], axis=2).squeeze()\n",
        "        print('attention =', attention_vector)\n",
        "        assert (np.sum(attention_vector) - 1.0) < 1e-5\n",
        "        attention_vectors.append(attention_vector)\n",
        "\n",
        "    attention_vector_final = np.mean(np.array(attention_vectors), axis=0)\n",
        "    # plot part.\n",
        "    import matplotlib.pyplot as plt\n",
        "    import pandas as pd\n",
        "\n",
        "    pd.DataFrame(attention_vector_final, columns=['attention (%)']).plot(kind='bar',\n",
        "                                                                         title='Attention Mechanism as '\n",
        "                                                                               'a function of input'\n",
        "                                                                               ' dimensions.')\n",
        "    plt.show()\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 20, 2)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 20, 32)       4480        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 32, 20)       0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 32, 20)       0           permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 32, 20)       420         reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "attention_vec (Permute)         (None, 20, 32)       0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "attention_mul (Multiply)        (None, 20, 32)       0           lstm_1[0][0]                     \n",
            "                                                                 attention_vec[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 640)          0           attention_mul[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 1)            641         flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 5,541\n",
            "Trainable params: 5,541\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Train on 270000 samples, validate on 30000 samples\n",
            "Epoch 1/1\n",
            "270000/270000 [==============================] - 79s 293us/step - loss: 0.0469 - acc: 0.9789 - val_loss: 1.1063e-04 - val_acc: 1.0000\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0021006  0.00224832 0.00232845 0.00316943 0.00244766 0.00284921\n",
            " 0.00230551 0.00224494 0.00214144 0.00359221 0.5933661  0.00631137\n",
            " 0.35692427 0.00275591 0.00277598 0.00260897 0.00253626 0.00201713\n",
            " 0.0025963  0.00267989]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00141073 0.00178805 0.00275525 0.00173163 0.00245729 0.0018055\n",
            " 0.00144646 0.00132575 0.00186575 0.00298784 0.5807278  0.00451919\n",
            " 0.37960935 0.00332004 0.00266829 0.00215269 0.00198537 0.00204486\n",
            " 0.00170232 0.00169585]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00195473 0.00194583 0.0025529  0.00250608 0.00189662 0.00197187\n",
            " 0.00170348 0.00213205 0.00164855 0.00304487 0.5917898  0.00476378\n",
            " 0.36683166 0.00236638 0.00163041 0.00321694 0.00189805 0.00181603\n",
            " 0.00192269 0.0024073 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00172676 0.00189346 0.00191034 0.00222821 0.00192541 0.00175217\n",
            " 0.00177448 0.00192495 0.00207234 0.00311673 0.62788105 0.00399551\n",
            " 0.32817692 0.00330694 0.00332865 0.00270005 0.00321681 0.00242593\n",
            " 0.00213479 0.00250853]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00088386 0.00090693 0.00108492 0.00115806 0.00104091 0.00095013\n",
            " 0.00103817 0.00077678 0.00096025 0.00211713 0.58929366 0.00227788\n",
            " 0.38979363 0.00195122 0.00107881 0.00115853 0.0006966  0.00086679\n",
            " 0.00099445 0.0009713 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0014523  0.00145673 0.00129165 0.00124939 0.00141777 0.0014276\n",
            " 0.00119652 0.00128462 0.00143194 0.00176347 0.5927236  0.00309434\n",
            " 0.37658927 0.00394122 0.00152525 0.00243269 0.001001   0.00164336\n",
            " 0.00129481 0.00178246]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.001537   0.00158668 0.00177698 0.00157796 0.00156529 0.00155235\n",
            " 0.00160702 0.00148583 0.00165293 0.00272719 0.5784149  0.00367517\n",
            " 0.3852632  0.00346533 0.00241712 0.00231356 0.00197662 0.00168431\n",
            " 0.00177795 0.00194257]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00161988 0.00156971 0.00172766 0.00150419 0.00153469 0.00159762\n",
            " 0.00159168 0.00144506 0.00156702 0.00245011 0.5908476  0.00337848\n",
            " 0.3748114  0.00357101 0.00178541 0.00240266 0.00152039 0.00154159\n",
            " 0.00168605 0.00184777]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0012226  0.00167492 0.00169266 0.00161788 0.00161704 0.0017868\n",
            " 0.00181372 0.00105503 0.0019387  0.0029949  0.57767636 0.0034362\n",
            " 0.38639855 0.00371422 0.00293885 0.00194083 0.0018078  0.00155185\n",
            " 0.00149904 0.00162207]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00088999 0.00192634 0.00156769 0.00145749 0.00139955 0.0013489\n",
            " 0.00148127 0.00123967 0.00176424 0.00359043 0.58531165 0.00300535\n",
            " 0.38225418 0.00306509 0.00184529 0.00197996 0.00088151 0.00183161\n",
            " 0.00170045 0.0014594 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00211181 0.00223067 0.0023986  0.00274753 0.00252566 0.00204991\n",
            " 0.00251346 0.0022946  0.00211837 0.00557224 0.59774214 0.00659488\n",
            " 0.34703064 0.00399116 0.00528031 0.0025761  0.00228373 0.00218338\n",
            " 0.00228361 0.0034712 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0011403  0.00135049 0.0013909  0.00149599 0.00146587 0.00123351\n",
            " 0.00138632 0.00130446 0.00132662 0.00278244 0.591429   0.00379462\n",
            " 0.37834796 0.00302542 0.00178542 0.00163061 0.00089518 0.00116344\n",
            " 0.00147382 0.00157766]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [7.7974203e-04 8.8294107e-04 9.6097973e-04 9.0313744e-04 9.4274583e-04\n",
            " 7.7810633e-04 8.4390811e-04 7.7799277e-04 8.7731332e-04 1.9160011e-03\n",
            " 5.9209657e-01 2.4110326e-03 3.8841146e-01 2.1027501e-03 9.0177689e-04\n",
            " 1.1573110e-03 5.1677949e-04 8.4995048e-04 8.8392157e-04 1.0055506e-03]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00166439 0.00151457 0.00149535 0.00145787 0.00144047 0.00144262\n",
            " 0.00129589 0.00145237 0.00150555 0.00212037 0.59107697 0.00309156\n",
            " 0.37566042 0.00343537 0.00154189 0.00306361 0.00150322 0.00184675\n",
            " 0.00143658 0.00195412]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00084875 0.00138343 0.00125523 0.00131969 0.00122082 0.00119215\n",
            " 0.00117383 0.00122942 0.00132957 0.00217849 0.5885819  0.00308702\n",
            " 0.38570783 0.00188239 0.00157602 0.00134142 0.00093627 0.00134649\n",
            " 0.00113149 0.00127771]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00073126 0.00085893 0.00096477 0.00109716 0.00090903 0.00082136\n",
            " 0.0009226  0.00078853 0.0008419  0.00221602 0.59091604 0.00233939\n",
            " 0.389583   0.00180559 0.00084343 0.00103436 0.00069235 0.00074864\n",
            " 0.0009918  0.00089384]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00107853 0.0012137  0.00143573 0.00140494 0.00134102 0.00115298\n",
            " 0.00118342 0.00105228 0.00135381 0.00269304 0.58721995 0.00297175\n",
            " 0.38496482 0.00265629 0.001751   0.00151383 0.00099729 0.00133057\n",
            " 0.00129152 0.00139345]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00170879 0.00179243 0.00198005 0.00171262 0.00166146 0.00170081\n",
            " 0.00163573 0.00161548 0.00170825 0.00244752 0.5772306  0.00393684\n",
            " 0.3854432  0.00332617 0.00211086 0.00248638 0.00187124 0.00170525\n",
            " 0.00184284 0.00208352]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00338263 0.00402989 0.00441124 0.00441764 0.00377256 0.00382003\n",
            " 0.00370005 0.00372471 0.00406236 0.00797487 0.5786151  0.01062395\n",
            " 0.33037323 0.00667328 0.00787953 0.00483115 0.00421095 0.00355342\n",
            " 0.00399284 0.00595054]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00088282 0.00120061 0.00119307 0.00128125 0.00126313 0.00099617\n",
            " 0.00116587 0.00122952 0.00147755 0.00220813 0.58487886 0.00241256\n",
            " 0.3894617  0.00238197 0.00142525 0.00142553 0.00114748 0.00139156\n",
            " 0.00143391 0.00114306]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00215247 0.00199709 0.00255013 0.00258904 0.00197621 0.00198388\n",
            " 0.0034098  0.00182256 0.00213768 0.00770969 0.5815723  0.00474143\n",
            " 0.3646571  0.00380599 0.00388593 0.00258416 0.00321693 0.00179039\n",
            " 0.00279579 0.00262149]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00249308 0.00407837 0.0077437  0.00265729 0.00387942 0.00333731\n",
            " 0.0027984  0.00259647 0.00396386 0.00543141 0.5330485  0.00649183\n",
            " 0.3908158  0.00358607 0.00671512 0.00350749 0.00506271 0.00349547\n",
            " 0.0043036  0.00399407]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00089346 0.00092979 0.00117788 0.00153705 0.0011492  0.00114465\n",
            " 0.00138649 0.00080422 0.0011559  0.0039254  0.5911572  0.00298567\n",
            " 0.38332254 0.00202917 0.00148642 0.00100872 0.0007857  0.0009801\n",
            " 0.00114175 0.00099864]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00065849 0.00068451 0.00074356 0.0006414  0.00066107 0.00066979\n",
            " 0.00064776 0.00063339 0.0007463  0.00105897 0.6171535  0.00149022\n",
            " 0.36712936 0.00180193 0.00090811 0.00117533 0.00083597 0.00078696\n",
            " 0.00068584 0.00088756]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00101965 0.00117302 0.00130231 0.00110026 0.00101371 0.00121167\n",
            " 0.00120292 0.00098702 0.00158099 0.00187819 0.5909248  0.00244007\n",
            " 0.3818409  0.0040975  0.0014065  0.00188663 0.00098475 0.001376\n",
            " 0.00118085 0.00139225]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00140461 0.00191709 0.00241024 0.00211484 0.00190438 0.00146421\n",
            " 0.00265426 0.00168592 0.0020358  0.00839679 0.5860379  0.00524174\n",
            " 0.36452353 0.00323692 0.00513537 0.00169095 0.00244527 0.00141127\n",
            " 0.00232903 0.00195989]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00150238 0.00169352 0.00190177 0.00184873 0.00151926 0.00179213\n",
            " 0.00185816 0.00146505 0.00173366 0.00277705 0.5885544  0.0033705\n",
            " 0.37554657 0.00261379 0.00182964 0.00228842 0.00222806 0.00153973\n",
            " 0.00207459 0.00186267]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00159744 0.00148422 0.00146631 0.00157276 0.00148208 0.00141623\n",
            " 0.00126687 0.00160697 0.00151506 0.00211781 0.5987332  0.00352914\n",
            " 0.36674786 0.00361931 0.00194688 0.00299478 0.00146108 0.00178858\n",
            " 0.00136437 0.00228909]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00157564 0.00130815 0.00135275 0.00130775 0.00137271 0.00130497\n",
            " 0.00119289 0.00150709 0.00149566 0.00175113 0.6003244  0.00323999\n",
            " 0.36642256 0.00398127 0.00192454 0.0028198  0.00175373 0.00188046\n",
            " 0.00136589 0.00211864]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00397789 0.00328401 0.00371943 0.00405765 0.00360564 0.00343979\n",
            " 0.00362319 0.00362318 0.00337211 0.00588421 0.5983906  0.00854439\n",
            " 0.3192019  0.00632117 0.00642393 0.00556245 0.00426388 0.00378476\n",
            " 0.00364337 0.00527643]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00135085 0.00158504 0.00168985 0.00291009 0.00154872 0.0016098\n",
            " 0.0023566  0.00129048 0.00188896 0.00402326 0.58924854 0.00307611\n",
            " 0.37329376 0.00336776 0.00195836 0.00206747 0.0018317  0.00139419\n",
            " 0.00169871 0.0018097 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00171053 0.00190087 0.00307424 0.00196191 0.00256307 0.0020692\n",
            " 0.00165048 0.00153721 0.00188172 0.00278894 0.58664525 0.00466755\n",
            " 0.37209332 0.00336429 0.00234531 0.00224379 0.00163756 0.00183853\n",
            " 0.0022341  0.0017921 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00135805 0.00174914 0.00155976 0.0019125  0.00155394 0.00188627\n",
            " 0.00185227 0.00141467 0.00180688 0.00255786 0.5925177  0.00377508\n",
            " 0.3701774  0.00332265 0.00301659 0.0019761  0.00192454 0.00157972\n",
            " 0.0016208  0.00243807]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00200223 0.0020713  0.00210685 0.00258348 0.00226274 0.00207458\n",
            " 0.00196965 0.00215169 0.00189996 0.0033182  0.59697217 0.00556644\n",
            " 0.3561855  0.00398281 0.00305407 0.00314266 0.00161556 0.00198514\n",
            " 0.00191176 0.00314314]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00152228 0.00131981 0.00240682 0.0018088  0.00133963 0.00150636\n",
            " 0.00203934 0.00094344 0.00141622 0.00649196 0.5828867  0.00340732\n",
            " 0.37898183 0.00289591 0.003423   0.00154956 0.00115556 0.00133916\n",
            " 0.00169094 0.00187534]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0019305  0.00239503 0.00264658 0.00334936 0.00232576 0.00203293\n",
            " 0.00366558 0.00204586 0.00263543 0.00563059 0.58331686 0.00388172\n",
            " 0.36273396 0.00375922 0.00301372 0.00308209 0.003615   0.00219325\n",
            " 0.00339966 0.00234686]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00230854 0.00262327 0.00273788 0.00327232 0.00274731 0.00236416\n",
            " 0.00257823 0.00291965 0.00264863 0.00636625 0.5834672  0.00820644\n",
            " 0.35353005 0.00474455 0.00488391 0.00317524 0.00242395 0.0026012\n",
            " 0.0025288  0.00387246]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00381457 0.00310297 0.00496255 0.00568663 0.0034854  0.00603332\n",
            " 0.00369238 0.00305314 0.00384643 0.00501634 0.55767334 0.01125467\n",
            " 0.3583187  0.00650833 0.00450005 0.0045417  0.0035114  0.00303661\n",
            " 0.00380135 0.00416017]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00349631 0.00248977 0.00503781 0.00213462 0.00267516 0.00216783\n",
            " 0.00233713 0.00213571 0.00234869 0.00408858 0.57493895 0.00610477\n",
            " 0.3630033  0.00567713 0.00491576 0.00427375 0.00281367 0.00242302\n",
            " 0.00269533 0.0042427 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00128968 0.001796   0.00260387 0.00166271 0.00170373 0.0016788\n",
            " 0.00180781 0.00140925 0.00185712 0.0050535  0.58325106 0.00536297\n",
            " 0.37350655 0.00270563 0.00352489 0.00181311 0.00346959 0.00164102\n",
            " 0.00149759 0.00236514]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00133504 0.00138036 0.00191285 0.00111914 0.00133956 0.0011462\n",
            " 0.00138626 0.00140339 0.00151032 0.0026498  0.5846306  0.00307588\n",
            " 0.38230416 0.00287783 0.00237461 0.00190026 0.00191765 0.00174646\n",
            " 0.00174067 0.00224898]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00180819 0.00147475 0.00155927 0.00154073 0.00152954 0.00159117\n",
            " 0.00137155 0.00172206 0.0015709  0.00202706 0.5930495  0.00360135\n",
            " 0.37238982 0.00319296 0.00147519 0.00303753 0.00154814 0.00182977\n",
            " 0.00163747 0.00204305]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00120457 0.00134003 0.00186992 0.00142224 0.00127005 0.00119483\n",
            " 0.00158057 0.00124046 0.00125401 0.00478521 0.58565086 0.00329964\n",
            " 0.3819641  0.00258659 0.00195291 0.00175908 0.00127514 0.00113014\n",
            " 0.00152043 0.00169919]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00202285 0.00202685 0.00216016 0.00232828 0.00226229 0.00193438\n",
            " 0.002487   0.00180782 0.00224582 0.00508495 0.5854883  0.00454493\n",
            " 0.36609083 0.00515159 0.00345851 0.00251565 0.00169143 0.00200171\n",
            " 0.00242618 0.00227052]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00113855 0.00114024 0.00155033 0.00115315 0.00116019 0.00113054\n",
            " 0.00128049 0.00116836 0.00132616 0.00210724 0.58031666 0.00267841\n",
            " 0.39089113 0.00274861 0.00236913 0.00160693 0.00155505 0.00152258\n",
            " 0.00139442 0.00176181]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00156467 0.00211035 0.00202136 0.00180058 0.00216348 0.00147796\n",
            " 0.00184026 0.00181216 0.00221049 0.00340274 0.57721364 0.00451687\n",
            " 0.38221788 0.00305747 0.00297014 0.00210825 0.00219735 0.00153479\n",
            " 0.00216737 0.00161222]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00189526 0.00206065 0.00220024 0.00251765 0.00215727 0.00178272\n",
            " 0.00248984 0.00204337 0.00187229 0.00654503 0.58918154 0.00525628\n",
            " 0.36081183 0.00332271 0.00415004 0.00227221 0.00222168 0.00188971\n",
            " 0.00237765 0.00295204]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00156483 0.00213748 0.00221263 0.00190327 0.00197094 0.00180125\n",
            " 0.0017432  0.00201682 0.00171575 0.00292909 0.60017776 0.00461267\n",
            " 0.35882872 0.00294173 0.00258714 0.00275553 0.00190511 0.00164187\n",
            " 0.00189934 0.00265487]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0018085  0.00162371 0.00190372 0.00253553 0.00190709 0.00166064\n",
            " 0.00182993 0.00193407 0.00182156 0.00418236 0.59048355 0.00508147\n",
            " 0.36779213 0.00291362 0.00254785 0.00223922 0.00161961 0.00203972\n",
            " 0.00189002 0.00218565]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00197432 0.00193729 0.00205821 0.00244358 0.00221456 0.00198661\n",
            " 0.00179079 0.00206036 0.00177374 0.00276205 0.5999782  0.00533115\n",
            " 0.3559885  0.00372837 0.00262699 0.00309741 0.00165301 0.00193697\n",
            " 0.00184782 0.00281012]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00122469 0.00123016 0.00127091 0.00113828 0.00118931 0.00113864\n",
            " 0.0010327  0.00113177 0.00125731 0.00185674 0.5933447  0.00272875\n",
            " 0.37931    0.00353177 0.00127109 0.00228727 0.00086887 0.00144697\n",
            " 0.00113374 0.00160631]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00089546 0.00094958 0.00103241 0.00101035 0.00091154 0.0009515\n",
            " 0.00086444 0.00087024 0.0010222  0.00157173 0.59207344 0.00244384\n",
            " 0.38631418 0.00272799 0.00100041 0.00151155 0.00060323 0.00110942\n",
            " 0.00084771 0.0012888 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00160146 0.00153913 0.00187699 0.00163079 0.00173646 0.00143214\n",
            " 0.00160832 0.00152727 0.00151836 0.00293986 0.5861434  0.00432161\n",
            " 0.37747067 0.00303364 0.00241725 0.0020516  0.00173122 0.0015292\n",
            " 0.00170635 0.00218429]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00202515 0.00183465 0.00211264 0.00252    0.00214061 0.00180695\n",
            " 0.00212106 0.00227861 0.00209999 0.00420008 0.62845784 0.00526142\n",
            " 0.32225245 0.00375172 0.00446258 0.00263469 0.00257098 0.00231704\n",
            " 0.0020963  0.00305524]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00103785 0.00109278 0.00107761 0.00115964 0.00108209 0.00104493\n",
            " 0.00103218 0.00101421 0.00106438 0.00182069 0.5905402  0.00272245\n",
            " 0.38497248 0.00258251 0.00151932 0.00150859 0.00089903 0.00113665\n",
            " 0.00108557 0.00160692]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00101183 0.0016467  0.00177605 0.00133453 0.00138125 0.00130881\n",
            " 0.0012631  0.0012524  0.00195999 0.00278643 0.5811374  0.00314256\n",
            " 0.38720298 0.00297861 0.00221667 0.00170264 0.0011301  0.00176191\n",
            " 0.00146497 0.00154101]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00117905 0.00105693 0.00150024 0.00098106 0.00100237 0.00089265\n",
            " 0.00144618 0.00107741 0.0009947  0.00192379 0.5792843  0.00247816\n",
            " 0.39338303 0.00245053 0.0034307  0.00170706 0.00129431 0.00091992\n",
            " 0.0013119  0.00168574]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00103209 0.00154142 0.00147783 0.00223045 0.00146995 0.00167456\n",
            " 0.00161764 0.00122685 0.00184513 0.00384759 0.5830122  0.00395256\n",
            " 0.38073727 0.00410806 0.00250915 0.00184469 0.00117229 0.00162113\n",
            " 0.00136665 0.00171241]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00126398 0.00147729 0.00159846 0.00170113 0.00152226 0.00152086\n",
            " 0.0014475  0.00145829 0.0014344  0.00287407 0.58219457 0.00312067\n",
            " 0.38707906 0.00221967 0.0012966  0.0019739  0.00128363 0.00141461\n",
            " 0.00164793 0.00147114]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00112051 0.00151542 0.00177719 0.00160012 0.00137216 0.00150959\n",
            " 0.00121942 0.00116719 0.00154778 0.0028444  0.5873732  0.00304439\n",
            " 0.38224098 0.00289682 0.001537   0.00186308 0.0010444  0.00145713\n",
            " 0.00134067 0.00152855]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00174572 0.00163871 0.00203027 0.00212063 0.00175076 0.00188244\n",
            " 0.0017067  0.00186948 0.00158975 0.00277856 0.58951795 0.00506996\n",
            " 0.3702364  0.00300974 0.00238067 0.00233009 0.00187782 0.00170119\n",
            " 0.00177619 0.00298693]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00114564 0.00129645 0.00134801 0.0014612  0.00129844 0.0013166\n",
            " 0.00125551 0.00120465 0.00132372 0.00228738 0.58814394 0.00321815\n",
            " 0.3830213  0.00272642 0.00159457 0.00174379 0.00125354 0.00134413\n",
            " 0.00130124 0.00171531]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00171134 0.00201661 0.00200404 0.00220469 0.00227836 0.0018639\n",
            " 0.00179477 0.00202552 0.00184108 0.00369244 0.6049861  0.00631533\n",
            " 0.34914696 0.00359807 0.00389082 0.00222694 0.00164397 0.0019483\n",
            " 0.00184924 0.00296158]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00114727 0.00144694 0.0016545  0.00127599 0.00130757 0.00111153\n",
            " 0.00157167 0.00116684 0.00172551 0.00318075 0.5812218  0.00299822\n",
            " 0.38702908 0.00297589 0.0027398  0.00184716 0.00138121 0.00141381\n",
            " 0.00138484 0.0014196 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00221834 0.00222761 0.00230969 0.00337982 0.00271061 0.00234455\n",
            " 0.00245455 0.0024762  0.00219142 0.00576683 0.5919304  0.00771146\n",
            " 0.3524932  0.00339379 0.00438006 0.00238235 0.00183332 0.0023814\n",
            " 0.00232266 0.00309171]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00141472 0.00139589 0.00181315 0.00185201 0.00145075 0.0016782\n",
            " 0.0015046  0.0012432  0.00175573 0.00238091 0.5916772  0.00362079\n",
            " 0.37530136 0.00282964 0.00170958 0.0021088  0.00170282 0.0014668\n",
            " 0.00145543 0.00163846]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00158538 0.00162193 0.00252187 0.00234941 0.00152478 0.00152332\n",
            " 0.00298526 0.00138914 0.00167597 0.00838187 0.5886128  0.00333715\n",
            " 0.3670082  0.00249265 0.00274361 0.00171042 0.00269062 0.00145073\n",
            " 0.00251189 0.00188298]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00189843 0.0017382  0.00190892 0.0019226  0.00189006 0.00175405\n",
            " 0.00195042 0.0018996  0.00191741 0.00323592 0.5828164  0.00453682\n",
            " 0.3734032  0.0053314  0.00287663 0.00276026 0.0017009  0.001903\n",
            " 0.00203366 0.00252214]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00170279 0.00145522 0.00141239 0.00149093 0.0014543  0.00166329\n",
            " 0.00136373 0.00143522 0.00150934 0.0018397  0.59594977 0.00354099\n",
            " 0.3693467  0.00399944 0.00189572 0.00258224 0.00152118 0.00185713\n",
            " 0.00167102 0.00230891]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00124203 0.00113309 0.00138537 0.00149871 0.00133744 0.0011073\n",
            " 0.00140868 0.00125121 0.00126207 0.00289708 0.6187476  0.00346398\n",
            " 0.3519421  0.00243969 0.00207821 0.00154862 0.00128567 0.00115039\n",
            " 0.00132317 0.00149761]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00091829 0.00174945 0.00175755 0.00154818 0.00139908 0.00140769\n",
            " 0.00169975 0.00141143 0.00165737 0.00596489 0.5901159  0.00386333\n",
            " 0.37392426 0.0024561  0.00236724 0.00141534 0.00121305 0.00155907\n",
            " 0.00176922 0.00180285]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00105203 0.00112393 0.0011028  0.00102554 0.00107989 0.00114583\n",
            " 0.00102435 0.00098984 0.00119051 0.00155758 0.5918711  0.00271196\n",
            " 0.3821321  0.00404436 0.00136656 0.00185836 0.00090542 0.00121544\n",
            " 0.00103774 0.00156468]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00095771 0.0014975  0.00130727 0.00136652 0.00131373 0.00123439\n",
            " 0.00122491 0.00110992 0.00130577 0.00237463 0.58731925 0.00267174\n",
            " 0.38622034 0.00219569 0.0013371  0.00168318 0.00098813 0.00142054\n",
            " 0.00120921 0.00126247]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00237296 0.00216429 0.00283803 0.00304586 0.00218936 0.00249698\n",
            " 0.00259463 0.00214595 0.00263983 0.00423506 0.5900979  0.00489753\n",
            " 0.35713428 0.00349794 0.00249761 0.00321864 0.00422757 0.00240678\n",
            " 0.00292505 0.00237376]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00186598 0.00173384 0.0016383  0.00192006 0.00179034 0.00188301\n",
            " 0.00173297 0.00188178 0.00178278 0.00251588 0.58410144 0.0044869\n",
            " 0.3746938  0.00407317 0.00265532 0.00263248 0.00186126 0.00205382\n",
            " 0.00200765 0.00268916]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00214018 0.00301594 0.00259415 0.00299554 0.0025664  0.00278872\n",
            " 0.00339228 0.0015865  0.00237632 0.00489986 0.5689806  0.00352404\n",
            " 0.37758878 0.00440924 0.00311723 0.00388064 0.00330045 0.00239659\n",
            " 0.00205954 0.00238703]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00124947 0.00124423 0.00137521 0.00162049 0.00129888 0.00132391\n",
            " 0.00162485 0.00119107 0.00130013 0.00249193 0.5913955  0.0032135\n",
            " 0.3787641  0.00300365 0.0014482  0.00182728 0.00107215 0.00127787\n",
            " 0.00153152 0.00174613]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00199427 0.00180443 0.00233136 0.00390157 0.00213436 0.00278667\n",
            " 0.00282507 0.00229479 0.00225789 0.00679914 0.58236086 0.00910509\n",
            " 0.35823095 0.00416155 0.00483947 0.00185973 0.00299927 0.00169126\n",
            " 0.00245805 0.00316424]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00122037 0.0012818  0.00198954 0.00138967 0.00125891 0.00125927\n",
            " 0.00142423 0.00103431 0.00121365 0.0033809  0.58851844 0.0031521\n",
            " 0.38187695 0.00249891 0.00174454 0.00164406 0.00110081 0.00117648\n",
            " 0.00125543 0.00157956]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00217721 0.00260808 0.00259148 0.0028912  0.00271035 0.00258186\n",
            " 0.0026593  0.00262561 0.00255509 0.00415656 0.5922798  0.00701171\n",
            " 0.34996355 0.00432339 0.00398408 0.00318502 0.00281376 0.00248745\n",
            " 0.00286379 0.00353069]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0009568  0.00112273 0.00118229 0.00110338 0.00099431 0.00103838\n",
            " 0.00102588 0.00105913 0.00117561 0.00187519 0.5915818  0.00252041\n",
            " 0.38420358 0.00252302 0.00114593 0.00186853 0.00093324 0.00116921\n",
            " 0.00107718 0.00144338]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00139714 0.00150309 0.00151561 0.00147127 0.00140526 0.00133377\n",
            " 0.00135757 0.00124988 0.00146393 0.00238779 0.58516264 0.00336526\n",
            " 0.38322976 0.00361296 0.0018756  0.00195373 0.00128542 0.0013267\n",
            " 0.00134649 0.00175611]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00225886 0.00290356 0.00285776 0.003074   0.00297974 0.00245332\n",
            " 0.00267769 0.00303508 0.00268722 0.00468783 0.58835864 0.00597795\n",
            " 0.34998962 0.00427294 0.00456008 0.00369839 0.0039164  0.00282724\n",
            " 0.00286962 0.00391408]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0011588  0.00144856 0.00214783 0.00137007 0.00124663 0.00145283\n",
            " 0.00187009 0.0012855  0.00141925 0.003108   0.5806393  0.00311586\n",
            " 0.38595143 0.00248924 0.00291603 0.00180119 0.00141507 0.00144544\n",
            " 0.00158612 0.00213276]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00119612 0.0011107  0.0012148  0.00156351 0.0012448  0.00126413\n",
            " 0.00112354 0.00118529 0.00118445 0.0019993  0.62033975 0.00357167\n",
            " 0.3523505  0.00228382 0.00170002 0.00155307 0.00110259 0.00129343\n",
            " 0.00105791 0.00166058]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00208045 0.00216174 0.00243568 0.00340521 0.00237812 0.00207937\n",
            " 0.00260629 0.00234066 0.00212875 0.0069603  0.59333104 0.00662367\n",
            " 0.350353   0.00326139 0.00523601 0.00240375 0.0023806  0.0021232\n",
            " 0.0022109  0.00349991]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00170013 0.00154054 0.00186486 0.00177176 0.00154401 0.00157549\n",
            " 0.00188324 0.00139838 0.00213327 0.00277952 0.5936612  0.00286421\n",
            " 0.3690231  0.00450584 0.00207558 0.00259238 0.00167465 0.00184795\n",
            " 0.00198342 0.00158049]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00221129 0.00261031 0.00241037 0.00270465 0.00258379 0.00216487\n",
            " 0.00277611 0.00229104 0.0021251  0.0063935  0.59297    0.00561295\n",
            " 0.35153514 0.00385719 0.00461211 0.00280821 0.00249563 0.00200954\n",
            " 0.00256483 0.00326328]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00084133 0.00083093 0.00099736 0.0010292  0.00088988 0.00088986\n",
            " 0.0008818  0.00072727 0.00093234 0.0016836  0.5914029  0.00237159\n",
            " 0.38900834 0.00202424 0.00105787 0.00107069 0.00066649 0.00084609\n",
            " 0.00086491 0.00098334]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00200288 0.00229728 0.00229042 0.00277042 0.00259185 0.00238973\n",
            " 0.00204889 0.00257303 0.0023527  0.00347392 0.58044654 0.00713817\n",
            " 0.36780357 0.00381948 0.00324956 0.00294061 0.00241014 0.00216291\n",
            " 0.00217578 0.00306208]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00161403 0.00150968 0.00211144 0.00206408 0.00160542 0.00160379\n",
            " 0.00154027 0.00177617 0.00161272 0.00313585 0.5935576  0.00473234\n",
            " 0.3686177  0.00285266 0.0022318  0.00231355 0.00138923 0.00159111\n",
            " 0.00168995 0.00245061]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00114988 0.00171042 0.00148306 0.00199058 0.00170864 0.00155223\n",
            " 0.00161836 0.00146057 0.00148288 0.00404967 0.58776563 0.00408427\n",
            " 0.37739155 0.00287687 0.00197106 0.00175826 0.00118843 0.00144559\n",
            " 0.00161785 0.00169417]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00168647 0.00248264 0.00160546 0.00151519 0.00181342 0.00122973\n",
            " 0.00197052 0.00138066 0.00183467 0.00600726 0.5853683  0.00300073\n",
            " 0.37128264 0.00488506 0.00365225 0.00317443 0.00157823 0.00160703\n",
            " 0.00169736 0.00222799]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00113763 0.00112255 0.00119486 0.00116802 0.00110769 0.00110046\n",
            " 0.00101668 0.00111441 0.00114539 0.00189335 0.59084153 0.00290888\n",
            " 0.38347268 0.00287519 0.00120497 0.00200232 0.00084885 0.00121295\n",
            " 0.00107566 0.00155588]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00132138 0.00133706 0.00134008 0.00134003 0.00129112 0.00141274\n",
            " 0.00122788 0.00118078 0.00127374 0.00196996 0.5912329  0.00323274\n",
            " 0.37985784 0.00290092 0.00149526 0.00195825 0.00108805 0.00142023\n",
            " 0.00133342 0.00178566]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00094013 0.00125897 0.00126252 0.00196737 0.0012892  0.00144775\n",
            " 0.0016304  0.00091474 0.001385   0.00249043 0.5879916  0.00259282\n",
            " 0.3852925  0.00248427 0.00144055 0.00137798 0.00096774 0.00097022\n",
            " 0.00115923 0.00113659]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00083598 0.0009397  0.00104439 0.00096291 0.00092363 0.00094385\n",
            " 0.00097625 0.00082054 0.0010213  0.00164767 0.58903456 0.00213704\n",
            " 0.38944668 0.00258865 0.00130514 0.00128603 0.00090302 0.0010454\n",
            " 0.00099149 0.00114572]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00071969 0.00086383 0.00105487 0.00104766 0.00085536 0.00100301\n",
            " 0.0008633  0.00072206 0.00108127 0.00205404 0.58963984 0.00234349\n",
            " 0.38992622 0.00206208 0.00105065 0.00109414 0.00067893 0.00095047\n",
            " 0.00091304 0.00107603]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00171991 0.00146355 0.00177515 0.00202151 0.00178558 0.00174966\n",
            " 0.00156464 0.0014634  0.0018375  0.00252276 0.59523803 0.00371677\n",
            " 0.37035334 0.00270728 0.00177159 0.00211686 0.00150954 0.00155395\n",
            " 0.00165062 0.0014784 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00190351 0.00171781 0.00187943 0.00180886 0.00163084 0.00191134\n",
            " 0.00169606 0.00182342 0.00193943 0.00248536 0.5912944  0.00372583\n",
            " 0.36833102 0.00358718 0.00191647 0.00318114 0.00236584 0.0022005\n",
            " 0.00216486 0.00243675]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00174742 0.00165376 0.00173603 0.0019814  0.00176561 0.0017484\n",
            " 0.00170006 0.00182482 0.00177108 0.00285721 0.590653   0.00480756\n",
            " 0.3682506  0.00386823 0.00286014 0.00237278 0.00179259 0.00208001\n",
            " 0.00168609 0.00284322]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00115093 0.00116813 0.00151549 0.00141777 0.00129361 0.00114795\n",
            " 0.0013366  0.001456   0.0011628  0.00287119 0.5839658  0.00326482\n",
            " 0.3875284  0.00173033 0.00160474 0.00157021 0.00145721 0.0010772\n",
            " 0.0017875  0.00149334]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00198601 0.00233539 0.00269654 0.00277751 0.00189621 0.00191986\n",
            " 0.0031128  0.00275115 0.00243485 0.00651532 0.5852767  0.00326124\n",
            " 0.357771   0.003383   0.00345431 0.00397868 0.0041861  0.00338211\n",
            " 0.00302025 0.00386094]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00105986 0.00135472 0.00217458 0.00149447 0.00123137 0.00133239\n",
            " 0.00167639 0.00122896 0.00175207 0.00396913 0.58727634 0.00390117\n",
            " 0.3781898  0.00249824 0.00322378 0.00146013 0.00162569 0.00136497\n",
            " 0.00142131 0.0017646 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00134769 0.00129376 0.0012299  0.00120295 0.00127118 0.00121544\n",
            " 0.00121477 0.00117118 0.00126393 0.00193567 0.5906074  0.00287226\n",
            " 0.37997308 0.00378184 0.00172216 0.00214423 0.00115789 0.00141726\n",
            " 0.00136315 0.00181422]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00102507 0.00115919 0.00130939 0.00138228 0.00126026 0.00119696\n",
            " 0.00115749 0.00118592 0.00116745 0.00227329 0.59221953 0.00383236\n",
            " 0.38067955 0.00232304 0.00152207 0.00144109 0.00083772 0.00116874\n",
            " 0.0012083  0.00165029]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00137192 0.0012316  0.00150387 0.00175849 0.00137743 0.001323\n",
            " 0.00128081 0.00133957 0.00127798 0.00230953 0.59079164 0.00349739\n",
            " 0.37986743 0.00242688 0.00143605 0.00200439 0.00091244 0.00132039\n",
            " 0.00138974 0.00157942]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00148134 0.00161091 0.00187055 0.00163521 0.00166416 0.00147787\n",
            " 0.00173383 0.00170383 0.00176778 0.0035575  0.5862311  0.00433304\n",
            " 0.3747459  0.00377718 0.00308013 0.00216021 0.00132704 0.0019187\n",
            " 0.00184673 0.002077  ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0021001  0.00245677 0.00233272 0.00235971 0.00255342 0.00220311\n",
            " 0.00245735 0.0022737  0.00242461 0.00402809 0.60749507 0.00522606\n",
            " 0.33829254 0.00512394 0.00439341 0.00320646 0.00299459 0.00235165\n",
            " 0.00250758 0.00321907]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00161132 0.00141803 0.00136293 0.00154146 0.00149286 0.00133563\n",
            " 0.00128558 0.00157093 0.00154066 0.00200639 0.59860706 0.00330201\n",
            " 0.3665367  0.00402503 0.00216276 0.00287648 0.00177744 0.00195288\n",
            " 0.00137962 0.00221418]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00119257 0.00116292 0.0013045  0.00164715 0.00133133 0.00122631\n",
            " 0.00118897 0.00116256 0.00127488 0.00245943 0.5906629  0.00378397\n",
            " 0.38098916 0.00263537 0.00156109 0.00157354 0.00092737 0.00132714\n",
            " 0.00108499 0.0015038 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00099803 0.00105727 0.00107457 0.00107897 0.00109241 0.00094747\n",
            " 0.00097526 0.00101111 0.00105445 0.00188206 0.591102   0.00263402\n",
            " 0.3852064  0.002683   0.00126342 0.00164062 0.00080582 0.00109348\n",
            " 0.00102305 0.0013766 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00358698 0.00464335 0.003422   0.00634506 0.00510361 0.00328412\n",
            " 0.00452241 0.0037474  0.00340365 0.00808759 0.56780064 0.00592953\n",
            " 0.34785122 0.00463726 0.00405148 0.00647755 0.00551833 0.00322716\n",
            " 0.00434308 0.00401764]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00092228 0.00093503 0.00108818 0.0009901  0.00096842 0.00093072\n",
            " 0.00094947 0.00091668 0.00097892 0.00174349 0.5874349  0.00241465\n",
            " 0.39028376 0.0023115  0.00128506 0.00141689 0.00103382 0.00107166\n",
            " 0.00099726 0.00132723]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00144617 0.00159643 0.00159032 0.00154463 0.0015666  0.00147361\n",
            " 0.00164491 0.00138547 0.00154333 0.00292292 0.6058005  0.00311298\n",
            " 0.35910994 0.00310539 0.00242686 0.00214481 0.00207848 0.00164035\n",
            " 0.00191351 0.00195278]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0025912  0.00264609 0.00282253 0.00403304 0.00302852 0.00278722\n",
            " 0.00313587 0.00304967 0.00274498 0.00537392 0.5937121  0.00730009\n",
            " 0.33843565 0.00340484 0.00551523 0.00308288 0.00575989 0.0030698\n",
            " 0.00327048 0.004236  ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00135941 0.00177426 0.00226087 0.00238191 0.00178119 0.00220076\n",
            " 0.00248407 0.0016956  0.00231139 0.00760911 0.5841416  0.00591707\n",
            " 0.36657542 0.00376583 0.00369143 0.00162069 0.0020221  0.00173302\n",
            " 0.00223262 0.00244165]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00161007 0.00156673 0.00367285 0.00176581 0.00182872 0.00140544\n",
            " 0.00229208 0.00135044 0.00179679 0.00591866 0.5876229  0.00374228\n",
            " 0.36889678 0.00333177 0.00338657 0.00190001 0.00165488 0.00171522\n",
            " 0.00264963 0.00189237]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00173159 0.00142094 0.00190527 0.00185299 0.00186714 0.0017541\n",
            " 0.00186031 0.00211049 0.0015957  0.00237427 0.57625854 0.00534403\n",
            " 0.3840093  0.00273635 0.00267979 0.00210139 0.00251729 0.00165231\n",
            " 0.00212543 0.00210271]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00073153 0.00076686 0.00088142 0.00087137 0.00075053 0.00077314\n",
            " 0.0008784  0.00070766 0.00091618 0.00139327 0.5902026  0.00182243\n",
            " 0.39176953 0.00189043 0.00109043 0.00113281 0.00079529 0.0008066\n",
            " 0.00089408 0.0009254 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00364855 0.00421209 0.00436233 0.00419004 0.00408086 0.00372872\n",
            " 0.0035591  0.00386976 0.00391668 0.00613071 0.57401335 0.0093565\n",
            " 0.33712557 0.00784197 0.0066764  0.00647582 0.00352448 0.00366007\n",
            " 0.00418505 0.00544189]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00104465 0.00104608 0.00115759 0.00104031 0.0011441  0.00101111\n",
            " 0.00100101 0.00093961 0.00108054 0.00187988 0.58592105 0.00268462\n",
            " 0.38965294 0.00281039 0.00146584 0.00158962 0.00090706 0.0012646\n",
            " 0.00106238 0.00129667]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00140914 0.00155668 0.00163673 0.00189961 0.00167616 0.00141598\n",
            " 0.00154672 0.0015232  0.00155462 0.00365498 0.60593426 0.00454192\n",
            " 0.35648704 0.00323768 0.00358093 0.00182882 0.0013468  0.00150744\n",
            " 0.0013671  0.00229422]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00144549 0.00217375 0.00236096 0.00199908 0.00173544 0.00198843\n",
            " 0.00164434 0.00180273 0.00227307 0.00317072 0.58013433 0.00432175\n",
            " 0.37801096 0.00364921 0.0029354  0.00239589 0.00152157 0.00215609\n",
            " 0.00202428 0.00225658]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00169244 0.00170538 0.001851   0.00206911 0.00177079 0.00169992\n",
            " 0.00186557 0.0019471  0.00167761 0.00305082 0.59524065 0.00487399\n",
            " 0.36364353 0.00353285 0.00288988 0.00243425 0.00173932 0.00173421\n",
            " 0.00167944 0.00290216]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00144979 0.00155143 0.00143601 0.00160129 0.00165676 0.00162974\n",
            " 0.00140994 0.00160534 0.00175509 0.00214829 0.5911585  0.00409362\n",
            " 0.37194398 0.00443003 0.00229002 0.00232791 0.00168339 0.00193311\n",
            " 0.00160165 0.00229413]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00258497 0.00290814 0.00303155 0.00343983 0.00296017 0.00296724\n",
            " 0.00323541 0.00322498 0.00309158 0.00535017 0.60534793 0.00775291\n",
            " 0.32292116 0.00461148 0.00670398 0.0037406  0.00506943 0.00306655\n",
            " 0.00320955 0.00478231]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00098355 0.00129492 0.0014213  0.00141265 0.0011882  0.00146194\n",
            " 0.00128659 0.0010394  0.00156346 0.00228933 0.58327293 0.00293776\n",
            " 0.3882857  0.00269009 0.00203097 0.00155549 0.00124324 0.00133785\n",
            " 0.00113375 0.00157089]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00113974 0.0012069  0.0013636  0.00138286 0.00132763 0.00115428\n",
            " 0.00127296 0.00130248 0.00133948 0.00226036 0.6143173  0.00340747\n",
            " 0.35644668 0.0028245  0.00205485 0.0016971  0.00124789 0.00128245\n",
            " 0.00139921 0.00157234]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00168054 0.00213211 0.00243701 0.00231517 0.00190113 0.0018208\n",
            " 0.00283535 0.00183951 0.00215779 0.00870417 0.57901955 0.00448388\n",
            " 0.36919448 0.00374996 0.00381173 0.00260107 0.00265872 0.00175539\n",
            " 0.00246874 0.00243289]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00106585 0.00132298 0.0013783  0.00214379 0.00122716 0.0013977\n",
            " 0.00181658 0.00109713 0.00138124 0.00316591 0.59160805 0.00287444\n",
            " 0.37861925 0.00270825 0.00146952 0.00159256 0.00104295 0.00109787\n",
            " 0.00142729 0.00156323]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00145606 0.00168272 0.00189287 0.00157195 0.00174079 0.00140566\n",
            " 0.00151594 0.00153279 0.0015447  0.00327896 0.5866573  0.00406059\n",
            " 0.37827837 0.00285483 0.00223705 0.00199851 0.00126105 0.00153684\n",
            " 0.0016705  0.00182256]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00209378 0.00212504 0.00235888 0.00213863 0.00223626 0.00231817\n",
            " 0.00219761 0.00208798 0.00225454 0.00325127 0.61925435 0.00467749\n",
            " 0.32996073 0.00431164 0.0038836  0.0031338  0.00347042 0.00252994\n",
            " 0.00256056 0.00315525]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00164759 0.00159418 0.001758   0.00189668 0.00180436 0.00163748\n",
            " 0.00162343 0.00168984 0.00148724 0.00277854 0.58867216 0.00508428\n",
            " 0.37327817 0.00312891 0.00245459 0.00212    0.00163058 0.00159947\n",
            " 0.00161443 0.00250004]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00116654 0.00136081 0.00165866 0.00195143 0.00139962 0.00125156\n",
            " 0.00174007 0.00099934 0.00135677 0.00403935 0.5866691  0.00248946\n",
            " 0.38326922 0.00220327 0.00157722 0.00147196 0.00111121 0.00126614\n",
            " 0.0017475  0.00127076]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00155735 0.00163428 0.0019435  0.00196976 0.00159255 0.00164909\n",
            " 0.00192262 0.00180084 0.00211004 0.00371985 0.5979743  0.00405282\n",
            " 0.36100775 0.00372108 0.00309569 0.00223872 0.00174792 0.00190201\n",
            " 0.00210369 0.00225607]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00221984 0.00178454 0.00215209 0.00256491 0.00205652 0.00216777\n",
            " 0.00196332 0.00227412 0.00196956 0.0031135  0.62135637 0.00519306\n",
            " 0.33109173 0.00292935 0.0032047  0.00290912 0.00342955 0.00234522\n",
            " 0.00222269 0.00305204]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00086058 0.00094977 0.00098249 0.00103045 0.00092926 0.00091424\n",
            " 0.00085397 0.00079144 0.00095989 0.00163495 0.59150046 0.00244575\n",
            " 0.38800442 0.00218835 0.00109492 0.00126168 0.00061217 0.00100619\n",
            " 0.00081792 0.00116106]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0013918  0.00159965 0.00254844 0.00169632 0.00145814 0.00137895\n",
            " 0.00221782 0.00132864 0.00152117 0.00687337 0.58897805 0.00358917\n",
            " 0.3708067  0.00289706 0.00303138 0.00190295 0.00147899 0.00136254\n",
            " 0.00188771 0.00205116]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00123561 0.00192528 0.00170654 0.00223094 0.00163688 0.00160382\n",
            " 0.00174013 0.00194464 0.00198776 0.00456749 0.5816716  0.00420609\n",
            " 0.37818515 0.00361773 0.0021881  0.00216004 0.00130892 0.00201099\n",
            " 0.00216234 0.00190989]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00269745 0.00210884 0.00335695 0.00323957 0.00317567 0.00268332\n",
            " 0.00326157 0.00279078 0.00324627 0.00366215 0.5809174  0.00609845\n",
            " 0.35760665 0.00467821 0.00348435 0.00342208 0.00366569 0.00393326\n",
            " 0.0035245  0.00244681]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00149414 0.00153801 0.0017348  0.00210422 0.00170338 0.00167582\n",
            " 0.00201091 0.00159322 0.0016464  0.00271839 0.58705044 0.0038719\n",
            " 0.37617102 0.0031892  0.00179824 0.00223732 0.00165394 0.00170031\n",
            " 0.00216504 0.00194334]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00113846 0.0012062  0.00129456 0.00126032 0.00118432 0.00119125\n",
            " 0.00127694 0.00119151 0.00118099 0.00178859 0.6092832  0.00255939\n",
            " 0.36288893 0.00181546 0.00179724 0.00182879 0.00307853 0.00113198\n",
            " 0.00133272 0.0015706 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00141404 0.00121016 0.00174012 0.0012807  0.00131287 0.00139974\n",
            " 0.00122672 0.0011371  0.00144674 0.00172775 0.5934258  0.00319653\n",
            " 0.37732834 0.00358282 0.00137507 0.00207964 0.0010502  0.00127333\n",
            " 0.00129414 0.00149815]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00221774 0.00222177 0.00233158 0.0028027  0.00261248 0.00251106\n",
            " 0.00236831 0.00235776 0.00223016 0.00368441 0.6009934  0.00664922\n",
            " 0.34590536 0.00363548 0.00405381 0.00277556 0.00287503 0.00226657\n",
            " 0.00238498 0.00312263]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00152587 0.00136445 0.00143213 0.00160788 0.00137135 0.00147281\n",
            " 0.00122155 0.00150546 0.00148847 0.0020448  0.5908562  0.00386628\n",
            " 0.37585276 0.00353257 0.00168352 0.00261631 0.00131572 0.00173558\n",
            " 0.00128016 0.00222618]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00194107 0.00289806 0.00226491 0.00231716 0.00251387 0.00239319\n",
            " 0.00210121 0.00213939 0.00292282 0.00330539 0.57407486 0.00503301\n",
            " 0.3716281  0.00660161 0.00336882 0.00408015 0.00227137 0.00288357\n",
            " 0.0022396  0.00302185]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00236207 0.00206183 0.00276863 0.00263654 0.00247377 0.00207439\n",
            " 0.00314972 0.00206582 0.00255876 0.00576163 0.58895665 0.00408094\n",
            " 0.3583212  0.00367288 0.00232366 0.00307492 0.00363926 0.00266399\n",
            " 0.00332989 0.00202351]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00239555 0.0024657  0.00257987 0.0028169  0.00281257 0.00212474\n",
            " 0.00271013 0.00291676 0.00252427 0.00467939 0.61997825 0.0055682\n",
            " 0.31922168 0.00387741 0.0057051  0.00347895 0.00446517 0.00299619\n",
            " 0.00296489 0.00371831]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00097431 0.00116106 0.00112097 0.00105381 0.00108123 0.00099967\n",
            " 0.00099923 0.00098685 0.00109905 0.00171964 0.58689255 0.00252208\n",
            " 0.38902912 0.00283165 0.00142879 0.00168979 0.00075898 0.0011736\n",
            " 0.00107092 0.00140674]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00124804 0.00176867 0.00168418 0.0013431  0.00157292 0.0013774\n",
            " 0.00143838 0.00127829 0.00184923 0.00221256 0.58843637 0.00301285\n",
            " 0.37780035 0.00371637 0.00221042 0.00225738 0.00166596 0.0018586\n",
            " 0.00160115 0.00166782]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00090523 0.00119881 0.00136486 0.00135529 0.00110452 0.00107063\n",
            " 0.00117133 0.00093896 0.00119562 0.00251241 0.58922124 0.00250254\n",
            " 0.38623106 0.00222998 0.00125603 0.00143078 0.00096274 0.00104272\n",
            " 0.00106529 0.0012399 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00152045 0.00178539 0.00172524 0.00201693 0.0018874  0.00172135\n",
            " 0.0016202  0.0018893  0.0018705  0.00306709 0.590771   0.00531434\n",
            " 0.3690872  0.004237   0.00223675 0.00239864 0.00110298 0.0018103\n",
            " 0.00171053 0.00222741]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00257295 0.00331434 0.00320548 0.0045887  0.00240322 0.00370377\n",
            " 0.00333689 0.00218625 0.00318922 0.00490945 0.57968205 0.00416188\n",
            " 0.35844582 0.00501483 0.00277345 0.00434621 0.00368118 0.00218437\n",
            " 0.00343646 0.00286344]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0016874  0.00190933 0.00178253 0.00231251 0.00201672 0.0017856\n",
            " 0.00205462 0.00202094 0.00196369 0.00338671 0.6108024  0.00418015\n",
            " 0.34522974 0.00351986 0.00341769 0.00243542 0.00288128 0.00191233\n",
            " 0.00217064 0.00253045]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00250389 0.00220751 0.00248202 0.00321773 0.00248451 0.00246081\n",
            " 0.00253596 0.00238566 0.0022308  0.00468262 0.60081846 0.00550547\n",
            " 0.3435679  0.00327502 0.0032948  0.0032418  0.00455579 0.00256525\n",
            " 0.00293862 0.00304537]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00121517 0.00122668 0.0012403  0.00124755 0.00131203 0.0011085\n",
            " 0.00110371 0.00129884 0.00122371 0.00206063 0.5886023  0.00326461\n",
            " 0.3830197  0.00312087 0.00165187 0.00183888 0.00115438 0.00138531\n",
            " 0.00120005 0.00172497]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00173301 0.00232768 0.00227208 0.00277906 0.00254834 0.00202157\n",
            " 0.00195452 0.00223924 0.00206336 0.00505288 0.5897062  0.00678527\n",
            " 0.3607859  0.00339567 0.00343405 0.00234222 0.00173797 0.00199784\n",
            " 0.00212013 0.00270297]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00214553 0.00251294 0.00273535 0.00275776 0.00264061 0.00221547\n",
            " 0.00212934 0.00274878 0.00236344 0.00440193 0.6121641  0.00689118\n",
            " 0.33205003 0.00378685 0.00395934 0.00360856 0.00253096 0.0026042\n",
            " 0.00229988 0.00345378]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00339118 0.00624304 0.00509079 0.00333486 0.006211   0.00364196\n",
            " 0.00397361 0.01010642 0.0093981  0.00776702 0.48636    0.01637802\n",
            " 0.35154268 0.02056333 0.02274144 0.00753168 0.00815049 0.01009968\n",
            " 0.00800549 0.00946917]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00163521 0.00167592 0.00171812 0.0016777  0.00147783 0.00180775\n",
            " 0.00156216 0.00131024 0.00221916 0.0021819  0.5945646  0.00258771\n",
            " 0.36908323 0.00365178 0.00182    0.0029971  0.00208364 0.00258276\n",
            " 0.00156302 0.0018001 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00161062 0.00161065 0.00280581 0.00211931 0.00142558 0.00138527\n",
            " 0.00194107 0.00185973 0.00160104 0.00640165 0.58311856 0.00379505\n",
            " 0.3731572  0.00240056 0.00318462 0.0020829  0.00179266 0.00206639\n",
            " 0.0024274  0.00321393]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00130465 0.00119162 0.00137961 0.00150399 0.00135213 0.00134837\n",
            " 0.00121405 0.00136704 0.00132146 0.00214266 0.59181714 0.00418778\n",
            " 0.37749392 0.00306852 0.00167914 0.0018144  0.00103067 0.00157494\n",
            " 0.0012546  0.00195332]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00270145 0.00266229 0.00330995 0.00308063 0.00262904 0.00252578\n",
            " 0.00252187 0.00199906 0.002441   0.00687528 0.56467706 0.00534666\n",
            " 0.37754172 0.00369086 0.00333045 0.0035744  0.00383893 0.00228573\n",
            " 0.00217697 0.00279088]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00095135 0.00098238 0.00141877 0.00103374 0.00104355 0.00089235\n",
            " 0.00096102 0.00088266 0.00092664 0.00218735 0.5883651  0.00279993\n",
            " 0.38926446 0.00181451 0.00122856 0.00123606 0.00099465 0.00084795\n",
            " 0.00098466 0.00118439]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00099497 0.00110166 0.00122701 0.00121633 0.00111601 0.00104048\n",
            " 0.00102917 0.00099083 0.00120873 0.00214247 0.58776116 0.00267438\n",
            " 0.38732135 0.00271695 0.00147067 0.00152979 0.00088787 0.00123163\n",
            " 0.0010351  0.00130345]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00154018 0.00171427 0.00210945 0.00191387 0.0017205  0.00156117\n",
            " 0.00193994 0.00161933 0.00160685 0.00444117 0.5816887  0.00442129\n",
            " 0.37786862 0.00345313 0.00262734 0.00220896 0.0017754  0.00152091\n",
            " 0.00201701 0.00225188]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00289146 0.00395044 0.003542   0.00467426 0.00338457 0.0047618\n",
            " 0.00538795 0.00338274 0.00386444 0.00772929 0.5359736  0.00866315\n",
            " 0.37698194 0.00547618 0.0073573  0.00450197 0.00670212 0.00251739\n",
            " 0.00348709 0.00477029]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00166308 0.00170609 0.00180011 0.00226625 0.00211074 0.00182966\n",
            " 0.00155995 0.00187742 0.00176319 0.002801   0.6054505  0.00543271\n",
            " 0.35439152 0.00320887 0.00254216 0.0022864  0.00155621 0.00185392\n",
            " 0.00166717 0.002233  ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00160845 0.00182085 0.00218417 0.00157715 0.0015848  0.00144319\n",
            " 0.0016782  0.0019197  0.00180699 0.00309656 0.6000009  0.00322965\n",
            " 0.3600669  0.00256415 0.00217888 0.00340437 0.00386816 0.00183672\n",
            " 0.00201648 0.00211371]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00098902 0.00103866 0.00111938 0.00106492 0.00106026 0.0009573\n",
            " 0.00099351 0.00104777 0.00099704 0.00181135 0.5883002  0.00262085\n",
            " 0.38791525 0.00252306 0.00142596 0.00155512 0.00096656 0.00106903\n",
            " 0.00107962 0.00146511]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00085233 0.00101044 0.00102342 0.00105859 0.00100433 0.00094301\n",
            " 0.00099585 0.00087214 0.00095554 0.00179599 0.5916543  0.00277748\n",
            " 0.38671535 0.00222517 0.00124088 0.00119677 0.00067361 0.00082402\n",
            " 0.00099052 0.00119029]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0008408  0.00092886 0.00096371 0.0009477  0.00094335 0.00084208\n",
            " 0.00092616 0.00082548 0.00097762 0.00229414 0.5891285  0.00245542\n",
            " 0.38926753 0.00235644 0.00132432 0.00118996 0.00079129 0.00092032\n",
            " 0.0009122  0.00116411]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00168441 0.00183018 0.00231154 0.00176703 0.00164963 0.00178151\n",
            " 0.00196591 0.00134637 0.00139181 0.00390739 0.5815243  0.00374826\n",
            " 0.3800516  0.00261282 0.00252616 0.00226277 0.00197743 0.00137497\n",
            " 0.00164717 0.0026387 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00138963 0.00168792 0.00168523 0.00145691 0.00149981 0.00192459\n",
            " 0.00162488 0.00128708 0.00214093 0.00199419 0.59501094 0.00335822\n",
            " 0.36734772 0.00511635 0.00246042 0.00251788 0.00183345 0.00199235\n",
            " 0.00152287 0.00214864]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00155368 0.00274223 0.00313167 0.00256935 0.00246435 0.00258419\n",
            " 0.00218755 0.00263973 0.00365425 0.00373665 0.56942725 0.00552451\n",
            " 0.37629846 0.00416602 0.00345568 0.0028109  0.00183953 0.00310802\n",
            " 0.00340453 0.0027015 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00265697 0.00275208 0.00256231 0.00360837 0.00324698 0.00289107\n",
            " 0.00248296 0.00286807 0.00253741 0.00429715 0.59577435 0.00780379\n",
            " 0.34393787 0.00415199 0.00370467 0.00360417 0.00231436 0.00276652\n",
            " 0.00262421 0.0034147 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00093296 0.0010191  0.00110509 0.00111564 0.00101447 0.00113954\n",
            " 0.00097457 0.0010113  0.00120249 0.00158406 0.58634365 0.0024289\n",
            " 0.38998377 0.00254194 0.00114921 0.00154873 0.00105777 0.00141879\n",
            " 0.00102483 0.00140314]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00081921 0.00103658 0.00093704 0.00085026 0.00097061 0.00078718\n",
            " 0.00086131 0.00073956 0.00094769 0.00218633 0.59009206 0.00228841\n",
            " 0.38882244 0.00240237 0.00137032 0.00121909 0.00078144 0.00093982\n",
            " 0.00085096 0.00109723]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00164942 0.00287118 0.00257423 0.00382307 0.00268055 0.00259726\n",
            " 0.00344995 0.00173484 0.00302752 0.00400921 0.5753556  0.00370452\n",
            " 0.3726475  0.00390908 0.00317689 0.00271512 0.00335531 0.0017696\n",
            " 0.00297554 0.00197366]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00075758 0.00075274 0.00087471 0.00085968 0.00077706 0.00075813\n",
            " 0.00077346 0.00073364 0.00085969 0.00140651 0.59059644 0.00196791\n",
            " 0.39158642 0.00199797 0.00089108 0.00115373 0.00070085 0.0008154\n",
            " 0.00080863 0.00092834]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00116789 0.0012478  0.00144978 0.0013209  0.00118528 0.00138348\n",
            " 0.001289   0.00120433 0.00134567 0.00199284 0.59032524 0.00283343\n",
            " 0.38149184 0.00278818 0.00134174 0.00208562 0.00132618 0.00130211\n",
            " 0.00140351 0.0015152 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00116017 0.00133702 0.00151787 0.0021615  0.00126096 0.00131022\n",
            " 0.00204106 0.00113502 0.00141016 0.00315105 0.59062374 0.00268378\n",
            " 0.37788892 0.00265884 0.00153356 0.00160172 0.00204031 0.00116173\n",
            " 0.00182099 0.00150141]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00155137 0.00153421 0.0020138  0.00165321 0.00148781 0.00145364\n",
            " 0.00141037 0.00177815 0.00158678 0.00260158 0.600422   0.00321277\n",
            " 0.36434788 0.00224129 0.00198009 0.00262273 0.00286475 0.00172412\n",
            " 0.00164669 0.00186675]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00112337 0.00115685 0.00125877 0.00120318 0.00117607 0.00114728\n",
            " 0.00120416 0.00109392 0.00131017 0.00196839 0.587554   0.00278342\n",
            " 0.38484854 0.00359352 0.00164116 0.00170312 0.00124056 0.00130652\n",
            " 0.00127497 0.00141199]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00167192 0.00151186 0.00188663 0.00168337 0.00148072 0.00162122\n",
            " 0.00156581 0.00128775 0.0022267  0.00250224 0.590399   0.00301535\n",
            " 0.37290376 0.00417173 0.00197696 0.0027234  0.00178109 0.00230028\n",
            " 0.00150154 0.00178861]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00067883 0.00081852 0.0010723  0.00089894 0.0008052  0.00077791\n",
            " 0.00102832 0.00070557 0.00080165 0.0026587  0.5916663  0.00237297\n",
            " 0.38865653 0.00155622 0.0012774  0.00092363 0.00068207 0.00071591\n",
            " 0.00092715 0.00097588]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00599316 0.0074509  0.01549731 0.00540625 0.00726523 0.00455831\n",
            " 0.00537773 0.00539787 0.0060268  0.01450975 0.53288555 0.01413127\n",
            " 0.31562543 0.00810446 0.01537906 0.00820102 0.00723402 0.00521928\n",
            " 0.00807993 0.00765666]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00103441 0.00105108 0.00125192 0.00164362 0.00113063 0.00113284\n",
            " 0.00115932 0.00086899 0.00107669 0.00250255 0.5851943  0.00250516\n",
            " 0.3908849  0.0020272  0.00134417 0.00130855 0.00091924 0.00084423\n",
            " 0.00110775 0.00101241]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0008361  0.00081138 0.00099143 0.00095995 0.00086612 0.00085685\n",
            " 0.000792   0.00086784 0.00082135 0.00148934 0.591247   0.00283721\n",
            " 0.38911825 0.00184459 0.00096554 0.00113675 0.00064638 0.00086317\n",
            " 0.00082928 0.00121939]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00179346 0.00207934 0.00224199 0.00211767 0.00195708 0.00221618\n",
            " 0.00208434 0.00164965 0.00238737 0.0031149  0.5947333  0.00406478\n",
            " 0.3600095  0.00404399 0.00319501 0.00262468 0.00267038 0.00223433\n",
            " 0.00231922 0.0024628 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00110417 0.00106132 0.00123861 0.00115215 0.00106903 0.00123522\n",
            " 0.00093828 0.00098663 0.00115414 0.00149732 0.5923508  0.00267792\n",
            " 0.38286707 0.00281967 0.00124723 0.0018089  0.00086285 0.00143568\n",
            " 0.00094344 0.00154961]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00295152 0.0035349  0.00352614 0.00361333 0.00356783 0.00307328\n",
            " 0.00352921 0.0034834  0.00332756 0.00803925 0.58626205 0.00846955\n",
            " 0.33209544 0.00624953 0.0085669  0.00411925 0.00367428 0.00324434\n",
            " 0.00371197 0.00496028]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00230595 0.00234058 0.00291264 0.00302975 0.00275073 0.00249156\n",
            " 0.00216822 0.00260846 0.00230769 0.00498189 0.5972987  0.00914991\n",
            " 0.34395623 0.00404508 0.00448864 0.002831   0.00189016 0.00254426\n",
            " 0.00224056 0.00365806]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00122355 0.00175682 0.00162451 0.00233281 0.00148171 0.00148874\n",
            " 0.00244929 0.00127868 0.00176383 0.00445122 0.5873709  0.00287086\n",
            " 0.37678337 0.00256304 0.00211314 0.00176297 0.00182314 0.00126293\n",
            " 0.00196234 0.00163613]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00079927 0.00092681 0.00109059 0.00112606 0.00093848 0.00091\n",
            " 0.00097464 0.00076331 0.00108939 0.00255349 0.5902728  0.00253691\n",
            " 0.38755795 0.00239685 0.00123047 0.00120392 0.0006432  0.00096806\n",
            " 0.00090544 0.00111238]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00212165 0.00298027 0.00214965 0.00373688 0.00314079 0.0021942\n",
            " 0.00318228 0.00358237 0.00362602 0.00509632 0.5696698  0.0055472\n",
            " 0.36633372 0.00512089 0.0045505  0.00337012 0.00414748 0.00259215\n",
            " 0.00409039 0.00276733]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00158761 0.00150081 0.00149049 0.00150324 0.00159215 0.00158389\n",
            " 0.00137982 0.00152552 0.00147822 0.00221554 0.58998394 0.00401193\n",
            " 0.37546614 0.00383836 0.00184187 0.00242768 0.00132248 0.00165571\n",
            " 0.00146292 0.0021317 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0011981  0.00118391 0.00129538 0.00119832 0.00122614 0.00119237\n",
            " 0.00111511 0.0011385  0.00122027 0.00185857 0.5899794  0.00310322\n",
            " 0.38243294 0.00319398 0.00147144 0.00187565 0.00105718 0.00141013\n",
            " 0.00121005 0.00163927]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00102096 0.00114553 0.00139113 0.00112952 0.00115376 0.00106156\n",
            " 0.00101963 0.00102735 0.00116911 0.00189765 0.5912608  0.00294157\n",
            " 0.3839764  0.00266576 0.0012436  0.00161535 0.0007564  0.00109981\n",
            " 0.001084   0.00134005]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00110069 0.00101724 0.00122198 0.00144592 0.0012192  0.0011289\n",
            " 0.00098507 0.00119737 0.00108032 0.00175345 0.61870587 0.00357465\n",
            " 0.35573712 0.00204176 0.00142249 0.00158995 0.00106322 0.00117653\n",
            " 0.00100322 0.00153503]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00106727 0.00125136 0.00116425 0.00102145 0.00109761 0.00121532\n",
            " 0.0010527  0.00097431 0.00144755 0.00153477 0.60728014 0.00224983\n",
            " 0.3661804  0.00363697 0.00156371 0.00199581 0.00106537 0.00162977\n",
            " 0.0010602  0.00151119]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00187531 0.0019837  0.00229239 0.00215683 0.00224236 0.00183948\n",
            " 0.00182504 0.00216777 0.00185125 0.00357448 0.5807238  0.00509138\n",
            " 0.37477598 0.00364697 0.00236543 0.00283352 0.00188268 0.00193982\n",
            " 0.00255613 0.00237564]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00159648 0.00170811 0.00162495 0.00164212 0.00162629 0.00161345\n",
            " 0.00151787 0.0017791  0.00169238 0.00235432 0.58880365 0.00405113\n",
            " 0.37306917 0.00417172 0.00219898 0.0028238  0.00156427 0.00190397\n",
            " 0.00171662 0.00254165]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00200329 0.00174685 0.00214842 0.00230981 0.00198924 0.0019775\n",
            " 0.00189316 0.00218641 0.001769   0.00307674 0.5894028  0.00608885\n",
            " 0.364206   0.00390598 0.00324919 0.00268651 0.00208087 0.0020163\n",
            " 0.00182516 0.00343797]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00195596 0.00191599 0.00207337 0.00323075 0.00225655 0.00202395\n",
            " 0.00189554 0.00218585 0.00209246 0.00400712 0.6322113  0.00577934\n",
            " 0.3194629  0.00321401 0.003881   0.00255957 0.00208847 0.00239792\n",
            " 0.0018191  0.00294884]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00101216 0.00143692 0.00147505 0.00164449 0.001256   0.00136269\n",
            " 0.00140198 0.00103035 0.00122514 0.00248479 0.5906349  0.00283538\n",
            " 0.38192767 0.00236868 0.00143589 0.00157074 0.0010956  0.0010577\n",
            " 0.0012065  0.0015374 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00074647 0.00105181 0.00123344 0.0009663  0.00098997 0.00093042\n",
            " 0.00117879 0.00080005 0.00104565 0.00311698 0.58997816 0.00249016\n",
            " 0.38707167 0.00203439 0.00152136 0.00103392 0.00077569 0.00088156\n",
            " 0.00111315 0.00104009]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00190088 0.00191872 0.00221352 0.00228625 0.00202308 0.00173202\n",
            " 0.00206653 0.00203391 0.00219281 0.00429356 0.59435135 0.00457256\n",
            " 0.3594918  0.00462754 0.00275293 0.00315551 0.00183238 0.00206237\n",
            " 0.00222551 0.00226678]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00243445 0.0027677  0.00273782 0.00317194 0.00290684 0.00252095\n",
            " 0.00292482 0.00278885 0.00289921 0.00577179 0.58945584 0.00754811\n",
            " 0.3459019  0.00544445 0.00433713 0.00373283 0.00302138 0.00286188\n",
            " 0.00322163 0.00355044]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00263359 0.00374818 0.00658282 0.00319034 0.00432204 0.00287925\n",
            " 0.00320203 0.0049643  0.00432396 0.00574114 0.56054306 0.01149666\n",
            " 0.35219342 0.00467447 0.00781742 0.00407313 0.00452929 0.0042387\n",
            " 0.00498372 0.00386249]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00149979 0.00120908 0.00144318 0.00127464 0.00148073 0.00110277\n",
            " 0.001735   0.00105712 0.00141379 0.00260072 0.58934134 0.00255651\n",
            " 0.3790549  0.00385088 0.00244467 0.00181071 0.00179    0.00132403\n",
            " 0.00161735 0.00139273]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0011217  0.00137206 0.00124678 0.00120301 0.00114237 0.00092891\n",
            " 0.00130756 0.00087711 0.00121793 0.00434744 0.58956903 0.00228904\n",
            " 0.38248086 0.00260099 0.00209841 0.00149332 0.00100972 0.00107436\n",
            " 0.00120195 0.00141744]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [5.6452682e-04 6.7850511e-04 8.7485381e-04 7.1681745e-04 6.6110992e-04\n",
            " 6.6061050e-04 7.2213809e-04 5.5186369e-04 7.0363993e-04 1.7095328e-03\n",
            " 5.9251678e-01 1.8301196e-03 3.9223009e-01 1.5947996e-03 7.1280805e-04\n",
            " 8.2870200e-04 3.9265861e-04 6.1481749e-04 7.1056141e-04 7.2507840e-04]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00083509 0.00092523 0.00097077 0.0009632  0.00096843 0.00088887\n",
            " 0.00092911 0.00085958 0.0008987  0.00200123 0.59101546 0.00270106\n",
            " 0.3880959  0.00194583 0.0011788  0.0011308  0.00072806 0.00086523\n",
            " 0.00092223 0.00117649]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00275504 0.00259787 0.00271136 0.00390298 0.00291062 0.00283604\n",
            " 0.00266788 0.00272146 0.00245001 0.0045086  0.59429014 0.00672034\n",
            " 0.34665605 0.00364819 0.00363479 0.003586   0.00263949 0.00254488\n",
            " 0.00268085 0.00353741]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00207762 0.00228259 0.00244378 0.00229354 0.00249975 0.00225364\n",
            " 0.00247166 0.00170079 0.0022747  0.0030532  0.5742923  0.00543767\n",
            " 0.37620217 0.00411372 0.00412851 0.00274783 0.00330273 0.00174226\n",
            " 0.00257496 0.00210659]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00103737 0.00109586 0.0011661  0.00117524 0.00113732 0.00101569\n",
            " 0.00123529 0.00096771 0.0012136  0.0024103  0.58988345 0.00268877\n",
            " 0.3839354  0.00317508 0.00178564 0.00147847 0.00099411 0.00111068\n",
            " 0.00118775 0.0013062 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00178454 0.00202981 0.00204721 0.00242611 0.00226656 0.00226467\n",
            " 0.00173433 0.00213057 0.00209148 0.00298282 0.5970044  0.00662984\n",
            " 0.35713196 0.00400962 0.00251945 0.00287422 0.00156939 0.00202201\n",
            " 0.00179977 0.00268122]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00100341 0.00121007 0.00122629 0.00130701 0.00124099 0.00110724\n",
            " 0.00120996 0.00111932 0.00127569 0.0026573  0.587468   0.00319093\n",
            " 0.38564613 0.00247891 0.00166446 0.0014192  0.00102478 0.00113356\n",
            " 0.00120546 0.00141131]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00233994 0.00229639 0.00278642 0.00315283 0.00260449 0.00262868\n",
            " 0.00228556 0.00246725 0.00251393 0.00484587 0.591515   0.00752098\n",
            " 0.3499061  0.00394263 0.00467794 0.00284869 0.00265736 0.00294789\n",
            " 0.00248435 0.00357772]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00076326 0.00083558 0.00119543 0.00090465 0.00079771 0.00085697\n",
            " 0.00088666 0.00068681 0.00085964 0.00222769 0.59037036 0.00210715\n",
            " 0.39000767 0.00195816 0.00098159 0.0011003  0.00060919 0.00090204\n",
            " 0.0009391  0.00101002]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00130722 0.00130642 0.00160641 0.00141944 0.00117604 0.00136785\n",
            " 0.00126952 0.00097842 0.00149223 0.00244169 0.59094644 0.00267804\n",
            " 0.38077396 0.00251897 0.00146318 0.0017993  0.00117733 0.0015017\n",
            " 0.00129231 0.00148354]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00177019 0.00154816 0.00167938 0.00169237 0.00136909 0.00155136\n",
            " 0.00192765 0.00239486 0.00181446 0.00391695 0.57367736 0.00400454\n",
            " 0.38120425 0.0034427  0.00354777 0.0024177  0.00415456 0.00220615\n",
            " 0.00230024 0.00338023]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0024738  0.0026865  0.00319125 0.00344998 0.00301852 0.00274009\n",
            " 0.00261632 0.00288517 0.00275282 0.0047707  0.5796053  0.0080617\n",
            " 0.35670283 0.00590599 0.00385366 0.00351873 0.00257224 0.00282284\n",
            " 0.00286226 0.00350926]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00128763 0.00117686 0.00122257 0.00117816 0.00123763 0.00115882\n",
            " 0.00115757 0.0010765  0.00129449 0.0017727  0.5906017  0.00278886\n",
            " 0.3811455  0.00425802 0.0016507  0.00197338 0.00101557 0.00126789\n",
            " 0.00121947 0.00151599]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00101083 0.00104318 0.00110575 0.00132821 0.00108159 0.00115442\n",
            " 0.00110398 0.00105575 0.00107109 0.00180711 0.5914445  0.00305649\n",
            " 0.38418916 0.00260083 0.00107361 0.00151255 0.00084544 0.00110821\n",
            " 0.00106423 0.00134308]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00166143 0.00145082 0.00193859 0.00161154 0.00152546 0.00148295\n",
            " 0.00171951 0.00167421 0.00178126 0.00342658 0.6008635  0.00352679\n",
            " 0.36067483 0.00375523 0.00224944 0.00281121 0.0022844  0.0016994\n",
            " 0.00189302 0.00196983]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00093217 0.00091234 0.00102126 0.00122179 0.0009873  0.00094693\n",
            " 0.0009728  0.00091623 0.00094148 0.00175439 0.588531   0.0028426\n",
            " 0.3892941  0.00215606 0.00124685 0.00139357 0.00080944 0.00093534\n",
            " 0.00099598 0.00118835]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00155484 0.00210065 0.00197188 0.00207226 0.00195263 0.00173972\n",
            " 0.00211336 0.00185432 0.00192403 0.0043427  0.5843892  0.00510011\n",
            " 0.37056774 0.00360682 0.00414396 0.00219771 0.00204157 0.00167476\n",
            " 0.00193572 0.0027161 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0017116  0.00202298 0.00219004 0.00210166 0.00198323 0.00199009\n",
            " 0.00175141 0.00191107 0.0020194  0.00326109 0.5873625  0.004945\n",
            " 0.3691281  0.00440899 0.0025649  0.00260422 0.00163702 0.0020066\n",
            " 0.00193145 0.00246862]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [6.7918398e-04 8.9020946e-04 9.3915267e-04 8.8737858e-04 8.6469686e-04\n",
            " 7.4623426e-04 9.2729775e-04 6.7706942e-04 8.7903882e-04 2.4701699e-03\n",
            " 5.9182286e-01 2.1721981e-03 3.8913953e-01 1.8829687e-03 1.0011565e-03\n",
            " 9.9554961e-04 5.0359283e-04 7.3814119e-04 8.9253922e-04 8.9100434e-04]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00084846 0.00084384 0.00102072 0.00089156 0.00087814 0.00085429\n",
            " 0.0008976  0.0009139  0.00083046 0.00154701 0.5896847  0.00248166\n",
            " 0.39039347 0.00200571 0.00106249 0.00116504 0.0007156  0.00087735\n",
            " 0.0009217  0.0011663 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00202011 0.00184008 0.00211906 0.00234695 0.00220905 0.00180056\n",
            " 0.00194679 0.00213144 0.00209221 0.00409741 0.6256238  0.00519928\n",
            " 0.3264566  0.00422797 0.00354198 0.00286218 0.00242487 0.00239292\n",
            " 0.00196594 0.00270071]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00154079 0.00243687 0.00224948 0.00238146 0.00193209 0.00203791\n",
            " 0.00177595 0.00173457 0.00221459 0.00347334 0.5835554  0.00447012\n",
            " 0.37453967 0.00359169 0.00227301 0.0025982  0.00143926 0.00188697\n",
            " 0.00168707 0.0021816 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00164217 0.00183649 0.00227306 0.00180571 0.00188358 0.00172712\n",
            " 0.00150089 0.00161009 0.00180089 0.00306732 0.5838111  0.00489948\n",
            " 0.37630385 0.00412344 0.00226553 0.00257919 0.00125412 0.00165632\n",
            " 0.00182965 0.00213   ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00178913 0.0016849  0.00156126 0.00165749 0.00182536 0.00159381\n",
            " 0.00151896 0.00186135 0.00176534 0.00232388 0.5935984  0.0043437\n",
            " 0.36512324 0.00520053 0.00293517 0.00285938 0.00182478 0.0021312\n",
            " 0.00167438 0.00272768]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00177095 0.00168937 0.00183529 0.00205035 0.00175496 0.00176461\n",
            " 0.00173888 0.00181985 0.0019021  0.00295946 0.58995473 0.00428135\n",
            " 0.36882073 0.00364367 0.00239044 0.00263628 0.00229661 0.0022608\n",
            " 0.0019247  0.00250482]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00140659 0.00136049 0.00141056 0.00167718 0.00141104 0.00138488\n",
            " 0.00148906 0.0015171  0.00139819 0.00245355 0.58525    0.00401796\n",
            " 0.3803988  0.00336786 0.002261   0.0020239  0.00191963 0.00146695\n",
            " 0.00145241 0.00233282]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00263105 0.0023621  0.002789   0.00299121 0.0026305  0.00232144\n",
            " 0.00265863 0.00271772 0.00246688 0.00545013 0.6062151  0.00699268\n",
            " 0.33208603 0.0046097  0.00520692 0.0035495  0.00319819 0.00267707\n",
            " 0.00236572 0.00408038]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00226387 0.0017214  0.00211797 0.00189741 0.00163147 0.00203669\n",
            " 0.00190492 0.00178791 0.00157809 0.00303976 0.5671574  0.00467677\n",
            " 0.38901153 0.00383678 0.00285661 0.0030835  0.00233156 0.0017565\n",
            " 0.00180459 0.00350528]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00229439 0.00260958 0.00275181 0.00319026 0.00249209 0.00228655\n",
            " 0.00479605 0.00223821 0.00268077 0.00882518 0.5797279  0.00485322\n",
            " 0.35597634 0.00437779 0.00459878 0.00294396 0.00457827 0.00223776\n",
            " 0.00351115 0.00302991]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00122805 0.00133545 0.00128208 0.00121157 0.00144231 0.00123095\n",
            " 0.00110519 0.00113254 0.00134276 0.00192227 0.61575973 0.00325827\n",
            " 0.35499346 0.00343897 0.00206098 0.00183444 0.00101445 0.00151515\n",
            " 0.00114219 0.00174917]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00112931 0.00114028 0.00129546 0.00114542 0.00121458 0.00114152\n",
            " 0.00118628 0.00099641 0.00135643 0.00179871 0.6114778  0.00255007\n",
            " 0.36203444 0.00308981 0.0018388  0.00159983 0.00126854 0.0012122\n",
            " 0.00121929 0.00130478]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00107324 0.00117654 0.00145625 0.00124192 0.00127078 0.00105833\n",
            " 0.00117496 0.00109077 0.00120407 0.00222002 0.59143686 0.00310485\n",
            " 0.38190144 0.00279631 0.00148237 0.00156888 0.00078413 0.00116865\n",
            " 0.00127576 0.00151388]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00201862 0.00228815 0.00258715 0.00235263 0.00213162 0.00191024\n",
            " 0.00238714 0.00200705 0.00222285 0.00453976 0.58995736 0.00407431\n",
            " 0.36268872 0.00326171 0.0025914  0.00311707 0.00284706 0.00204574\n",
            " 0.00266295 0.00230847]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00158153 0.00182018 0.00151149 0.00183694 0.00171112 0.00181483\n",
            " 0.00204362 0.00175005 0.00178895 0.0026986  0.58658093 0.00336319\n",
            " 0.3722473  0.00363364 0.00378019 0.00238567 0.00294413 0.00196222\n",
            " 0.00190832 0.0026371 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00138224 0.00144828 0.00158486 0.00166419 0.00143366 0.00135445\n",
            " 0.00151007 0.00138702 0.00142039 0.00353452 0.59050775 0.0041876\n",
            " 0.375754   0.00302185 0.00213456 0.00181399 0.00107765 0.00135231\n",
            " 0.0014777  0.00195293]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00280364 0.00294376 0.00345126 0.00377459 0.00328155 0.00311668\n",
            " 0.00297006 0.00332892 0.00304869 0.00620297 0.5834835  0.01088873\n",
            " 0.34283674 0.00507542 0.00563133 0.00374427 0.00295824 0.00303593\n",
            " 0.00287827 0.00454543]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00097542 0.0010886  0.00119451 0.0012331  0.00106031 0.00104326\n",
            " 0.00103725 0.00098906 0.00110329 0.00217195 0.58878785 0.00256052\n",
            " 0.38727587 0.00230464 0.00123111 0.00148394 0.00091139 0.00116992\n",
            " 0.00114126 0.00123673]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00128569 0.00120936 0.00136313 0.0010952  0.00116334 0.00118481\n",
            " 0.0010691  0.00120886 0.00125002 0.00177375 0.59172046 0.00289549\n",
            " 0.380032   0.00358165 0.00124276 0.00253631 0.00115975 0.00134569\n",
            " 0.00118415 0.00169846]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00115518 0.00138618 0.00163809 0.00132899 0.00132701 0.00130667\n",
            " 0.00145931 0.00121106 0.00141098 0.00220137 0.5836845  0.00292864\n",
            " 0.3852784  0.00288057 0.00249462 0.00204698 0.00179527 0.00142087\n",
            " 0.00136583 0.00167949]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00136968 0.00143955 0.00198426 0.00169637 0.0015263  0.00143355\n",
            " 0.00151508 0.00130566 0.00133638 0.00383249 0.59126526 0.00420167\n",
            " 0.37525457 0.00257825 0.00192247 0.00172    0.00091646 0.00139681\n",
            " 0.00145137 0.00185384]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00190504 0.00158157 0.00251888 0.00310978 0.00219244 0.00249344\n",
            " 0.00256847 0.00180125 0.00229007 0.0028071  0.5877589  0.00481835\n",
            " 0.36659822 0.0033249  0.00216292 0.0022716  0.0023044  0.00246601\n",
            " 0.00309655 0.00193008]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00199119 0.0017307  0.00237963 0.0023737  0.00217213 0.001768\n",
            " 0.00271138 0.00178869 0.0022627  0.00484847 0.58714867 0.0039056\n",
            " 0.3666158  0.0037555  0.00221259 0.00284877 0.00252914 0.00255071\n",
            " 0.00257101 0.00183555]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00133736 0.00131543 0.00140939 0.00128407 0.00129696 0.00125454\n",
            " 0.00126996 0.00128036 0.00141636 0.00204817 0.59083027 0.00309851\n",
            " 0.37882522 0.00372634 0.0016676  0.00220718 0.00120522 0.0013905\n",
            " 0.00138328 0.00175333]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00079705 0.00081042 0.00123076 0.00090012 0.00076895 0.00075897\n",
            " 0.00087976 0.00098323 0.00078732 0.00158363 0.5877616  0.00222353\n",
            " 0.39228064 0.00147711 0.00133863 0.00115657 0.00096279 0.00079462\n",
            " 0.00102876 0.00147549]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00108409 0.0011837  0.00127777 0.0011616  0.00111576 0.00119061\n",
            " 0.00115764 0.00099318 0.00121798 0.00182312 0.5900367  0.00238006\n",
            " 0.384593   0.00296727 0.00119515 0.00176606 0.00106152 0.00119428\n",
            " 0.00130625 0.00129424]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00136879 0.00153762 0.00229638 0.00217283 0.00151523 0.00163079\n",
            " 0.00280556 0.00118245 0.00190084 0.00747704 0.5851202  0.0039674\n",
            " 0.369996   0.00348776 0.00458601 0.00162057 0.00170382 0.00138474\n",
            " 0.00206416 0.00218173]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00178331 0.00173635 0.00180032 0.00185402 0.00181796 0.00160844\n",
            " 0.0018155  0.00191805 0.00177699 0.00332368 0.5839075  0.00425656\n",
            " 0.3744973  0.00384268 0.00300996 0.00271024 0.00194733 0.00198166\n",
            " 0.00184914 0.00256298]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00103642 0.00131545 0.00138478 0.00133212 0.00118766 0.00124925\n",
            " 0.00136229 0.00118684 0.00136318 0.00222045 0.588656   0.0026713\n",
            " 0.38421115 0.00233993 0.00146298 0.00161465 0.0013466  0.00117062\n",
            " 0.00158976 0.00129856]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00284588 0.00283519 0.00349247 0.00286149 0.00296263 0.00297681\n",
            " 0.00266985 0.00292673 0.00294183 0.00401344 0.56851006 0.00662937\n",
            " 0.36616045 0.00629821 0.00402767 0.00467746 0.00312456 0.00292588\n",
            " 0.00326349 0.00385659]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00218565 0.00274653 0.0028009  0.00292999 0.00260092 0.00216963\n",
            " 0.00234941 0.00279663 0.00250763 0.00578609 0.5943842  0.00712358\n",
            " 0.34555966 0.00450393 0.00519384 0.00326548 0.0021854  0.00249763\n",
            " 0.002248   0.00416489]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00264923 0.00311954 0.00183546 0.00239813 0.00252358 0.00179057\n",
            " 0.00343235 0.00275803 0.00278162 0.00933225 0.58456874 0.00389113\n",
            " 0.3462398  0.0066627  0.00613832 0.00442491 0.00369457 0.00453684\n",
            " 0.00359243 0.00362981]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00176141 0.00211018 0.00228178 0.00192291 0.00197575 0.00191963\n",
            " 0.00212322 0.00179652 0.00228116 0.00282011 0.59581506 0.00393334\n",
            " 0.36012158 0.00385931 0.0027547  0.00303252 0.00309342 0.00193272\n",
            " 0.00224708 0.00221757]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00179152 0.00169272 0.00220211 0.00202808 0.0016957  0.00228286\n",
            " 0.00204683 0.00146887 0.0021575  0.00262438 0.5939108  0.00423532\n",
            " 0.36383963 0.00395121 0.00271188 0.00252703 0.00281885 0.00174727\n",
            " 0.00196776 0.0022997 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00077593 0.00079709 0.00108713 0.0009255  0.00085705 0.0008236\n",
            " 0.00085225 0.00068952 0.0008419  0.00173391 0.5890577  0.00197398\n",
            " 0.39270574 0.00157555 0.00093722 0.00103976 0.00080961 0.00077991\n",
            " 0.00090904 0.00082765]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00192528 0.00215326 0.00218821 0.00168504 0.00249036 0.00157255\n",
            " 0.0015123  0.00189742 0.00220061 0.00339085 0.5776503  0.00476117\n",
            " 0.3771687  0.00519748 0.00316952 0.00299007 0.00130142 0.00193166\n",
            " 0.002717   0.0020968 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00091318 0.00117955 0.00137072 0.0013307  0.00121966 0.0009489\n",
            " 0.00121603 0.00117439 0.0011756  0.00352442 0.58977926 0.00323435\n",
            " 0.38369423 0.00214265 0.00143169 0.0013788  0.00090139 0.00096063\n",
            " 0.00125222 0.00117162]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00177794 0.00185423 0.00178262 0.00179205 0.0018907  0.00165359\n",
            " 0.00175265 0.00185317 0.00180963 0.00287853 0.583662   0.00463438\n",
            " 0.3722623  0.00499963 0.00411641 0.00268964 0.00189824 0.0020004\n",
            " 0.00183896 0.00285296]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00126473 0.00109588 0.00119985 0.00118603 0.00119637 0.00106291\n",
            " 0.00126227 0.0010399  0.00108314 0.00207011 0.58435154 0.00262177\n",
            " 0.3885657  0.00247396 0.00204006 0.00154809 0.00211385 0.00110563\n",
            " 0.00125176 0.00146646]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00209747 0.00164109 0.00222379 0.00207751 0.00175036 0.00165583\n",
            " 0.00206595 0.00198811 0.00286462 0.00353383 0.5856666  0.00346997\n",
            " 0.36706376 0.00418792 0.00289832 0.00336247 0.00344937 0.00349142\n",
            " 0.00234228 0.00216929]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00100339 0.00109864 0.00105509 0.00099906 0.00107288 0.00104129\n",
            " 0.00095749 0.00093007 0.00113566 0.00155822 0.5911473  0.00253403\n",
            " 0.38409543 0.00384734 0.00134918 0.00169356 0.00077689 0.001279\n",
            " 0.00096236 0.00146312]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00104563 0.00106549 0.00154439 0.00186808 0.00111105 0.00129132\n",
            " 0.00173868 0.00090769 0.00114058 0.00376705 0.59061205 0.00283617\n",
            " 0.38141698 0.00209858 0.00155133 0.00123814 0.00113267 0.00091907\n",
            " 0.00134606 0.001369  ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0039086  0.00236982 0.00376271 0.00391156 0.00349476 0.00283329\n",
            " 0.0034993  0.00287045 0.00243186 0.00588222 0.58082634 0.00796769\n",
            " 0.34822345 0.00413802 0.0036787  0.00383354 0.00683191 0.00259378\n",
            " 0.00375389 0.00318817]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00140285 0.00194623 0.00151908 0.00186373 0.00170352 0.00160706\n",
            " 0.00177803 0.00159374 0.00182834 0.0034765  0.58379006 0.00381782\n",
            " 0.3770375  0.00332191 0.0036265  0.00198793 0.00192859 0.00156516\n",
            " 0.00178333 0.00242209]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00202885 0.00170352 0.00197703 0.0021747  0.00193655 0.00169079\n",
            " 0.00229963 0.00194872 0.00210312 0.0034422  0.5925884  0.00394608\n",
            " 0.36292076 0.00380273 0.00303192 0.00278909 0.00267754 0.00229093\n",
            " 0.00241415 0.0022333 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00318581 0.00393703 0.00404309 0.0035456  0.00378254 0.00338741\n",
            " 0.00371387 0.00394889 0.00356517 0.00607955 0.59675735 0.00905453\n",
            " 0.3190497  0.00595038 0.00690097 0.00536583 0.00477549 0.00359616\n",
            " 0.00391399 0.00544668]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00181294 0.00191209 0.00198654 0.00216707 0.00194172 0.00181715\n",
            " 0.00190177 0.00215414 0.00192536 0.00349171 0.5842087  0.00486685\n",
            " 0.371357   0.00393371 0.00275821 0.00296485 0.00189836 0.00213089\n",
            " 0.00193967 0.00283125]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00181799 0.00203253 0.00221166 0.002332   0.00203803 0.00180914\n",
            " 0.00201758 0.00209913 0.00220609 0.00398808 0.5751591  0.005201\n",
            " 0.37797123 0.00427075 0.00347538 0.00272655 0.00174151 0.00206016\n",
            " 0.00218957 0.00265252]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00083326 0.00081943 0.00089516 0.00090393 0.00085794 0.000877\n",
            " 0.0008187  0.00074901 0.00087373 0.00145257 0.5909301  0.00199112\n",
            " 0.39072335 0.00180851 0.00091284 0.00118938 0.00065523 0.00090912\n",
            " 0.00087314 0.00092637]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00327283 0.00375468 0.00421418 0.0037982  0.00334667 0.00409504\n",
            " 0.00394868 0.00347337 0.00365115 0.00515823 0.60667455 0.00719111\n",
            " 0.31252986 0.00586586 0.00535983 0.00504894 0.00547738 0.00348104\n",
            " 0.00460063 0.0050578 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00112848 0.0012009  0.00141183 0.0013242  0.00129451 0.00114694\n",
            " 0.00119369 0.00120302 0.00125817 0.00216388 0.5874915  0.00347993\n",
            " 0.3847976  0.00255907 0.00177252 0.0015552  0.00109355 0.00118777\n",
            " 0.00117705 0.00156017]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00112712 0.00121088 0.00122837 0.00108925 0.00111781 0.00112861\n",
            " 0.00113514 0.00119216 0.00127795 0.00177315 0.5916664  0.00263555\n",
            " 0.3806895  0.00413083 0.00144337 0.00209818 0.00097677 0.0012643\n",
            " 0.0012805  0.00153411]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00147324 0.00147864 0.00183913 0.00159083 0.00152399 0.00172328\n",
            " 0.00170794 0.00135006 0.00206748 0.00223837 0.594212   0.00336066\n",
            " 0.3701179  0.00430249 0.00201001 0.00232049 0.00184762 0.0014765\n",
            " 0.00174348 0.00161592]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0012224  0.00131127 0.00137527 0.00121223 0.00121839 0.00131031\n",
            " 0.00115113 0.00115406 0.00166359 0.00189151 0.6079094  0.0027791\n",
            " 0.36175382 0.00391008 0.00181713 0.00223135 0.00127918 0.00190692\n",
            " 0.00116244 0.00174047]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00114867 0.00124882 0.00140023 0.00138171 0.00126934 0.00124969\n",
            " 0.00121423 0.00115533 0.00128181 0.00239804 0.58803713 0.00372431\n",
            " 0.38323984 0.00275308 0.00161391 0.00177444 0.00094218 0.00135154\n",
            " 0.00116296 0.00165276]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00136905 0.00145632 0.00184685 0.00171187 0.00152592 0.00149785\n",
            " 0.00216576 0.00156891 0.00156048 0.00393122 0.5821731  0.00407181\n",
            " 0.38118508 0.00245661 0.00275592 0.00160323 0.00226461 0.00136389\n",
            " 0.00186096 0.00163056]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00093221 0.00118842 0.00119285 0.00143742 0.00111642 0.00135302\n",
            " 0.00126639 0.00097651 0.00123932 0.00209312 0.5866083  0.00282423\n",
            " 0.3878725  0.00240227 0.00160888 0.00139267 0.00097948 0.00107638\n",
            " 0.00109593 0.00134365]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00198007 0.00216877 0.00213087 0.00302077 0.00232567 0.00236838\n",
            " 0.00216426 0.00236032 0.00222529 0.00410856 0.57542706 0.00577885\n",
            " 0.37267864 0.00417553 0.00344531 0.00288607 0.00298128 0.00234287\n",
            " 0.00221855 0.00321291]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00115038 0.00099352 0.00106821 0.00120303 0.00106194 0.00098179\n",
            " 0.00104942 0.00096412 0.00108823 0.00198929 0.58403516 0.00283318\n",
            " 0.3908458  0.00263011 0.00186382 0.00160975 0.00130891 0.00093615\n",
            " 0.00105177 0.00133538]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0020384  0.00190362 0.00213471 0.00262406 0.00237541 0.00201358\n",
            " 0.00200649 0.00213819 0.00185634 0.00397465 0.6055434  0.00651399\n",
            " 0.34670496 0.00318441 0.00389204 0.00229934 0.00186405 0.00206932\n",
            " 0.00194666 0.00291641]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00098483 0.00128745 0.00151907 0.00133556 0.00107176 0.00122708\n",
            " 0.00126594 0.00118694 0.00144383 0.00230099 0.5834968  0.00254195\n",
            " 0.38994724 0.00213358 0.00151462 0.00158701 0.0010983  0.00132545\n",
            " 0.00137376 0.0013578 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00318994 0.0034532  0.00342098 0.0039915  0.00351805 0.00300521\n",
            " 0.00410701 0.00367804 0.00340064 0.00814706 0.5961205  0.00666155\n",
            " 0.32250035 0.00526444 0.00664882 0.00470016 0.00555142 0.00374447\n",
            " 0.00420638 0.00469027]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00181952 0.00222731 0.00223513 0.00320852 0.00213963 0.00203715\n",
            " 0.00294308 0.00208688 0.00218615 0.00529665 0.5847371  0.00502312\n",
            " 0.36448807 0.00420746 0.00310435 0.00262856 0.00231028 0.00208088\n",
            " 0.00252095 0.00271917]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00094046 0.00094568 0.0010442  0.00096946 0.00095357 0.00089865\n",
            " 0.00092246 0.00086252 0.00098784 0.00161642 0.5911     0.00231859\n",
            " 0.38743138 0.00267968 0.00114293 0.0014157  0.00075432 0.0009303\n",
            " 0.000983   0.00110289]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00158498 0.00150555 0.00199471 0.00181854 0.00170461 0.00151423\n",
            " 0.00163089 0.00189572 0.00181579 0.00337327 0.5899763  0.00522095\n",
            " 0.3698593  0.00385995 0.00258507 0.00227427 0.00145557 0.00186415\n",
            " 0.00178174 0.0022844 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00141338 0.00142243 0.00159679 0.00236591 0.00158877 0.00154656\n",
            " 0.00129472 0.00158124 0.00127868 0.00276176 0.5925594  0.00486465\n",
            " 0.37448543 0.00222918 0.00157539 0.00188438 0.00089034 0.00135921\n",
            " 0.00125786 0.00204391]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00225655 0.00177007 0.00238478 0.00334397 0.00223462 0.00234428\n",
            " 0.00289417 0.00191654 0.00240664 0.00404485 0.579838   0.00435241\n",
            " 0.37032753 0.00376484 0.00249128 0.00276681 0.00290634 0.00273855\n",
            " 0.00307208 0.00214566]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00163355 0.00180875 0.00190851 0.00170744 0.00165375 0.00152157\n",
            " 0.00168599 0.00162414 0.00185319 0.00381288 0.5885612  0.00439363\n",
            " 0.3713417  0.00455383 0.00245626 0.00248201 0.00139085 0.00158698\n",
            " 0.00169087 0.00233289]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAELCAYAAAAiIMZEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaAO1nj32zy6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}